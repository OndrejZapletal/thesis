** Neural Networks
  Neural networks are definetely inspired by bilogy. First attemps to created model of neuron had multiple elelements equvialent with neurons of human brain. As time progressed this equivalence stopped beeing important and correspondence is only superficial.

 # gls:NN

In the first iterations of multiperceptron evolution it generaly had all neurons in hidden and output layer to be similar. Namely activation function was perdominantly sigmoid. Activation function of hidden layers is to this day one of the most dynamically evolving componenets of Neural networks. Currently there is several options for but mainly used is Restricted Linear Unit (ReLU) which models following mathematical function
\begin{equation}
g(z) = \max \{0,z\}
\end{equation}

#+CAPTION: Restricted Linear Unit (ReLU)
#+NAME:   fig:relu
#+ATTR_LATEX: :width 4in
[[./img/img__2__relu.png]]

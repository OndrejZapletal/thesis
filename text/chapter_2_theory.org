* Theory
  In this chapter will be discussed concepts of Artificial intelligence, Image processing, Machine learning in general, [[glspl:nn][NN]] and finally the [[glspl:cnn][CNN]].

** Artificial Intelligence
   Field of [[gls:ai][AI]] is very general and spawns across several different disciplines (e.g. mathematics, computer science, philosophy, economics and even ethics). This suggests that this field is very broad and can be tackled from many different view points. Therefore this description will not be be very exhaustive. For more broad and complex dive into subject refer to \cite{book--russell--2003}.

   It could be characterized by a pursuit of construction of intelligent agents. In other words attempt to create intelligent machines are either thinking or can be perceived as thinking ones.

   One of the most important abilities of intelligent agents is the sens of vision (even thought this is highly dependent on concrete application). Sense of vision is usually required to certain degree and not always it is necessary that it rivals the capabilities of for example human visual apparatus.

   First attempts to solve the vision problems were tackled from so called bottom up approach where system was instructed with hard-coded set of rules that were describing rules of vision. It was expected that as the understanding of mechanisms that allows humans to extract information from visual scene then these hard-coded systems can be fed more information in order to create more capable systems. Problem with this approach was that it highly underestimated the difficulty of formalization of these rules that could be prescribed into the intelligent system.

   This insight lead the researchers to postulating that in order to solve this problem there needs to be introduction of process that would allow the [[gls:ai][AI]] systems to extract patterns from provided data. In other words introduction of systems that are able to learn. Process that enables systems to learn is generally called [[gls:machine learning][machine learning]].

   There are several types of machine learning tasks. Most commonly encountered task is called [[gls:classification][classification]], which is the task to classify instance of input into correct discreet and predominantly finite class. Another typical type of machine learning task is called regression, which is based on the input data trying to estimate value of unknown quantity. Usually integer or real type.

   Rest of this document will be exclusively dealing with [[gls:classification][classification]] learning tasks.

   # Basics of image processing techniques.

** TODO Image Processing

   # There was no wider adoption of machine learning techniques in image processing for very long time, even though they existed as field of study since 1950's. Reason being that machine learning algorithms were very simple and therefore unfit for generally very complex problems of image processing (e.g. object detection and classification).

   Research in image processing was adopting predominately bottom up (feature-based) methodologies. Typical classification pipeline in computer vision would be following:
   - Image capture - Image is captured (by camera or similar device) and digitized.
   - Preprocessing - Digitized image is modified to emphasize important features (e.g. noise reduction, contrast normalization etc.).
   - Segmentation - Selection of interesting features (edges, similar surfaces).
   - Description - Extraction of radiometric, photometric descriptors and so on.
   - Classification - Some means to classify the object.


   # Classical approach to image processing is still very useful in very restricted environments with rigid constraints. One of examples can be detection of defects on line production in industrial automation.

   In the first attempts to apply machine learning in image processing, it was used usually deployed as the classification model. In other words complex problems were reduced and simplified in order to utilize machine learning techniques. Consequently this approach favors simpler models.

   This also brings the problem that any of these applications is not very versatile. Each application is usually only capable of solving very simple problem and any deviation from ideal circumstances can mean failure. Application can have problem with varying contrast, illumination, scaling, rotation etc.

   Second problem is often the fact that because image has to be preprocessed several times before it is fed into machine learning model it demands extra time and resources. This can created problems in training phase but also during operation after deployment.

   This is less of a problem with technical innovation and today it can be solved by very fast [[gls:dsp][DSP]], but it still is not negligible effect and it can have negative affect on the price of the solution. This is where [[gls:deep learning][DL]] models starts to exhibit significant advantage.

   [[Gls:deep learning][DL]] models are in theory capable of capturing complex (higher order) features of the input due to the fact they can learn very high level of abstraction from even very low level features. And what is even more important, is the fact that [[gls:deep learning][DL]] model in theory doesn't need any kind (or very little) of preprocessing. Input image can be directly connected as input into Deep Network.

   # Basics of Machine learning

** Machine Learning
   # TODO: REMOVE
   # It is necessary to properly define machine learning before we move forward. There are many valid definitions that may differ but in this text we will adhere to the following.
   # Field of Machine learning is a subset of [[gls:ai][AI]] that is describing tools that can be used to solve problems.

   As previously described Machine learning is a process that is used to create models that are able to extract information from data to solve given problem and consequently automatically improve their performance.

   Interesting perspective that can be used is to view machine learning as form of information compression. Where machine learning model is trying to extract information from input data in such a way that the amount of a data used to save it is reduced while the information contained within is preserved.

   There are typically two different types of machine learning approaches:
   - Unsupervised Learning
   - Supervised Learning

   # TODO: Maybe find better word for this???
   Both of these are typically used for different kinds of machine learning tasks.

**** Unsupervised learning
     In this learning approach the model is training by observing new data and extracting patterns in the date without being instructed of what they are. Opposed to supervised learning that is described bellow, the advantage of this approach is that the model is able to learn from data without supervision (as the name suggests). This means that there is no need for input data to be annotated, therefore it takes much less time and resources to deploy this model in practice.

     The biggest hurdle of supervised learning approach in real world applications is to obtain appropriate data. Appropriate data in this context mean, data that were somehow classified into different categories, which can be very tedious and slow process.

     Majority of unsupervised learning algorithms belong to group called clustering algorithms. These algorithms are centered around the idea to analyze geometric clustering of data in input space to determined their affiliation. This is achieved by the presupposition that data points clustering in input space are likely to exhibit similar properties. Examples of these models are:

     - [[gls:k-means][K-MEANS]] - clustering model \cite[p.~460--462]{book--hastie--2008};
     - [[glspl:som][SOM]] - instance based \cite{book--kohonen--2001};
     - [[gls:pca][PCA]] - dimensionality reduction \cite[p.~534--544]{book--hastie--2008}.

     Since unsupervised learning methods typically are not very useful for image classification, following text will be only dealing with supervised learning methods.

**** Supervised learning
     Supervised learning approach is more commonly used. This approach requires training data with specific format. Each instance has to have assigned label that is used to learn the algorithm. These labels provide the supervision for the learning algorithm and are used to estimate learning error.

     # TODO: add figure

     Supervised learning approach can be used to solve many different tasks. This document will concentrate only on the task of classification.

*** Structure of machine learning algorithm
    Even thought that machine learning algorithms are varied and are using different techniques its structure can be generalized. Structure of nearly all machine learning algorithms can be described using following components:
    - Dataset specification
    - Model
    - Cost function
    - Optimization procedure

    # TODO: Finish the thought
    A model of [[Gls:linear regression][Linear regression]] will be used as a case study to explain individual components mainly due to its simplicity. From the listed components the Dataset specification is usually the same for all different supervised learning algorithms. The other three components can vary dramatically. This level of analysis becomes useful for building of intuition for [[glspl:nn][NN]] and will be used to explain its individual components.

**** Dataset specification
     Supervised learning requires datasets with specific properties. Each dataset contains set of $n$ instances which consists of a pair of input vector $\boldsymbol{x}_i$ and output scalar $y_i$. Input vector

     \begin{equation}
     \boldsymbol{x}_i^T = [x_1, x_2, \dotsc, x_p],
     \end{equation}
     where $i$ is index of instance, $p$, is dimension of input vector.

     Individual components of input vector has to be of unified type. In case of input data in form of image it are values for individual pixels (e.g. 0-255), in other cases it can be real values. Almost universally in machine learning it stands that input should be normalized. This presumption holds in images automatically since each pixel has to have its vales in fixed range.
     It is very important in other types of machine learning tasks, where this is not guaranteed.

     Output scalar $y_i$ represents class of given instance. Type of this output value therefore has to acquire only certain values, in other words it has to be a set of cardinality equal to number of all possible classes.

**** Model
     Model is prediction apparatus that takes input $\boldsymbol{x}_i$ to predict value of it's output $y_i$. Each model has parameters represented by vector $\boldsymbol{\theta}$, which are adjusted during the training process. Probably the simplest examples of model of this type is linear model, also called [[gls:linear regression][linear regression]].

     Parameters $\boldsymbol{\theta}$ of this model are
     \begin{equation}
     \boldsymbol{\theta}^T = [\theta_1, \theta_2, \dotsc, \theta_p],
     \end{equation}
     where $p$ is number of parameters equal to size of input vector $\boldsymbol{x}_i$.

     Prediction $\hat{y}_i$ of the model on instance $i$ is computed as
     \begin{equation}
     \hat{y}_i =  \sum_{j=1}^{p} x_{ij} \theta_j.
     \end{equation}

     Therefore predictions of the model on the entire dataset in matrix notation is
     \begin{equation}
     \boldsymbol{\hat{y}} = \boldsymbol{X}\boldsymbol{\theta}.
     \end{equation}

     The same thing in expanded notation is equal to
     \begin{equation}
        \begin{bmatrix}
          \hat{y}_{1} \\
          \vdots      \\
          \hat{y}_{n}
        \end{bmatrix}
        =
        \begin{bmatrix}
          x_{11} & \cdots & x_{1p} \\
          \vdots & \ddots & \vdots \\
          x_{n1} & \cdots & x_{np}
        \end{bmatrix}
        \begin{bmatrix}
          \theta_{1} \\
          \vdots     \\
          \theta_{p}
        \end{bmatrix}.
     \end{equation}

     # TODO: This probably will not fit anywhere!!!
     # It most general case it can be viewed as model that is generating probability distribution.
     # \begin{equation}
     # p(y \mid \boldsymbol{x}; \boldsymbol{\theta})
     # \end{equation}

**** Cost function
     In order to achieve the learning ability of the machine learning algorithm it is necessary to estimate how correct the model is with its predictions. This is estimated with so called [[gls:cost function][cost function]] (also sometimes called [[gls:loss function][loss function]]).

     This function has to have certain properties. Ability of the machine learning algorithm to learn rests on the estimation of its improvement with change of its parameters. Therefore [[gls:cost function][cost function]] has be at least partially differentiable. For the case of linear regression it is most common to use [[gls:sum of square][sum of square]] error. The main reason being that derivative of this function for linear model has only one global minimum.

     [[Gls:cost function][Cost function]] is defined as
     \begin{equation}
     J(\boldsymbol{\theta}) = \sum_{i=1}^{n}{\left(y_i - \hat{y_i}\right)^2} =
     \sum_{i=0}^{n}{\left(y_i - \boldsymbol{x_i}^T \boldsymbol{\theta} \right)^2}.
     \end{equation}

     For the optimization purposes it is usually useful to express the [[gls:cost function][cost function]] in matrix notation
     \begin{equation}
     J(\boldsymbol{\theta}) = \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right)^T \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right).
     \end{equation}

**** Optimization procedure
     The last part of learning algorithm is the optimization procedure. It consist of update of model's parameters $\boldsymbol{\theta}$ in order to improve it's prediction. In other words to find $\boldsymbol{\theta}$ such that the value of [[gls:cost function][cost function]] $J(\boldsymbol{\theta})$ for given dataset is as small as possible.

     To investigate the change of [[gls:cost function][cost function]] on given dataset it is necessary to compute the derivative of $J(\boldsymbol{\theta})$ in respect to $\boldsymbol{\theta}$
     \begin{equation}
      \begin{split}
        \frac{\partial J(\boldsymbol{\theta})} {\partial \boldsymbol{\theta}} & = \frac{\partial} {\partial \boldsymbol{\theta}} \left[ \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right)^T \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right) \right] \\
        & = \frac{\partial} {\partial \boldsymbol{\theta}} \left[ \boldsymbol{y}^T \boldsymbol{y} + \boldsymbol{\theta}^T \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\theta} - 2\boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\theta} \right] \\
        & = 2\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\theta} - 2\boldsymbol{X}^T\boldsymbol{y}.
      \end{split}
     \end{equation}

     The optimal solution
     \begin{equation}
      \boldsymbol{\theta} = \left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y},
     \end{equation}
     is found by equating the partial derivative of $J(\boldsymbol{\theta})$ to $0$. Only condition is that $\boldsymbol{X}^T\boldsymbol{X}$ has to be non singular.
     # TODO: add some more sauce here !!!

*** TODO Model complexity

    In the first approximation it could be said that the task of supervised machine learning is to model relationship between the input output data most accurately. The problem with this is that in the real world problems there is never enough data to capture true relationship between the two. Therefore the task of machine learning is the attempt to infer true relationship by observing incomplete picture.

     Therefore the most important property of machine learning model is its generalization ability. That is ability to produce meaningful results on data that were not previously observed.

     # TODO: Try to find better image depicting the sam thing
     #+NAME: fig:over_under_fitting
     #+CAPTION: Figure shows different levels of generalization of model
     [[./img/figure__2__over_under_fitting.png]]

     Generalization ability is dependent on complexity of the model and its relationship to complexity of underling problem. When model doesn't capture complexity of the problem sufficiently it is described as under fitting. In cases that the complexity of model is exceeds the complexity of underling problem then this phenomenon is called over fitting.

     In both of these extremes the generalization ability suffers. In the former case the model is unable to capture true intricacies of the problem and therefore is unable to reliably predict desired output. In the latter case it tries to capture even the most subtle data perturbation that might be in fact a result of stochastic nature of the problem and not the real underlying relationship. This can be caused the fact that input data is missing some variable that is necessary to capture the true relationship. This fact is unavoidable and it therefore has to be taken into account when designing machine learning model. Depiction of theses phenomena in case of two variable input is on Fig. [[fig:over_under_fitting]].

     Typically the machine learning model is trained on as much of input data as possible in order to achieve the best performance possible. At the same time its error rate has to be verified on independent input data to check whether the generalization ability is not deteriorating. This is typically achieved by splitting available input data into training and testing set (usually in 4:1 ratio for training to test data). Model is trained with training data only and the performance of the model is tested on the test data. Relationship between test and train error can be found on Fig. [[fig:test_vs_training_error]]. Even though that the true generalization error can never be truly observed its approximation by test error rate is sufficient for majority of machine learning tasks.

     #+NAME: fig:test_vs_training_error
     #+CAPTION: Relationship between the model complexity and its ultimate accuracy is the relationship between training and testing error.
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__test_vs_training_error.png]]


**** Regularization
     As it was already mentioned, the most important aspect of machine learning is striking the balance between over and under fitting of the model. To help with this problem was devised concept of regularization. It is a technique that helps penalizes the model for its complexity.

     # TODO: You've used cost function here!!!
     Basic concept consists of adding a term in the [[gls:cost function][cost function]] that increases with model complexity.
     # TODO: TBD


** Neural Networks
   [[Gls:nn][NN]] is very general term describing broad family of models, due to a fact that its history goes back over 75 years. This situation often leads to confusion of some concepts. Model of [[gls:nn][NN]] is much more complex than the [[gls:linear regression][linear regression]] for this reason it is useful to divide the analysis into three parts:
    - model of neuron
    - topology of the network
    - learning algorithm

   Before the description of the model a brief history of [[glspl:nn][NN]] will be laid out in order to bring some terms into a context.

*** History

    # TODO: Try to find place for this or delete!!!
    # Perceptron model in [picture citation] has multiple inputs $x_1,...,x_n$ that are weighted $w_1,...,w_n$ and summed together. Additionally each neuron has bias $b$ that controls its weight.
    # Result is squashed through nonlinear activation function to produce output. Depending on the application, output is bounded between values 0 to 1 or -1 to 1.

    # TODO: assign citation
    # (citation) http://web.csulb.edu/~cwallis/artificialn/History.htm
    # https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png
    # https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png

    History of [[glspl:nn][NN]] can be arguably dated to 1943, when Warren Mcculloch and Walter Pitts devised mathematical model inspired by the biology of neuron cell found in brains \cite{article--mcculloch-pitts--1943}.

    This inspired the invention of [[gls:perceptron][Perceptron]], created in 1958 by Frank Rosenblatt. Perceptron used very simple model mimicking artificial neuron that was based on mathematical model of Pitts and Mcculloch. Model of [[gls:perceptron][Perceptron]] also encompassed an algorithm direct learning from data.

    # TODO: questionable update!!!
    # also add explanantion about equivalnce to FCNN.

    This simple improvement addressed majority of previously raised concerns and enable the application of [[glspl:nn][NN]] in many different technical domains with moderate success.

    From the beginning [[gls:perceptron][Perceptron]] seemed promising, but it was soon discovered that it had severe limitations. Most prominent voice of criticism was Marvin Minsky, who published book called Perceptrons \cite{book--minsky-papert--1969}. The criticism was centered on the fact that [[gls:perceptron][Perceptron]] model was unable to solve complex problems. Among others the book contained mathematical proof that [[gls:perceptron][Perceptron]] is unable to solve simple XOR problem. More generally the [[gls:perceptron][Perceptron]] is only capable to solve linearly separable problems. Even though according to Minsky this criticism wasn't malicious, it stifled the interest in [[glspl:nn][NN]] for over a decade.
    Interest in [[glspl:nn][NN]] was rejuvenated in the early 80's by the invention of [[gls:bp][BP]] learning algorithm, which enabled the possibility to use multiple [[glspl:perceptron][Perceptrons]] to form networks. [[Glspl:perceptron][Perceptrons]] could also be stacked upon each other to form layers. [[gls:nn][NN]] of this type are commonly called [[gls:mlp][MLP]].

    In 80's and 90's the interest in [[glspl:nn][NN]] plateaued again, and general research of [[gls:ai][AI]] was more focused on other (typically less complex) machine learning techniques. In the realm of [[gls:classification][classification]] problems there were several notable examples ([[gls:svm][SVM]], [[gls:ensemble models][Ensemble models]] and [[gls:random forest][Random Forest]]). [[gls:ai][AI]] research community also developed several other paradigms of [[glspl:nn][NN]] that were similarly inspired by biology of human brain but took different approaches. Most important examples were [[gls:som][SOM]] and [[gls:rnn][RNN]] (e.g. [[gls:hopfields networks][Hopfields Networks]]).

    By the year 2000 there was very few research groups that were devoting enough attention to the [[glspl:nn][NN]]. There was also certain disdain for [[glspl:nn][NN]] in academia and [[gls:ai][AI]] research community. Success of [[glspl:nn][NN]] that was promised almost half a century ago was finally encountered around 2006, when the first networks with large number of [[glspl:hidden layer][hidden layer]] was successfully trained. This led to mainstream adaption of umbrella term [[gls:deep learning][DL]] which typically refers to [[gls:dnn][DNN]]. The word [[gls:deep][Deep]] indicates that networks have large number of [[glspl:hidden layer][hidden layer]].

    The key theoretical insight was that to learn complicated functions that can represent high-level abstractions (e.g. vision recognition, language understanding etc.) there is a need for [[gls:deep][deep]] architecture.

    [[glspl:nn][NN]] in the times before [[glspl:dnn][DNN]] typically had only 1 or 2 [[glspl:hidden layer][hidden layer]]. These are today often called shallow networks. Typical Deep Networks can have number of [[glspl:hidden layer][hidden layer]] in order of tens, but in some cases even hundreds [citation]
    # https://www.microsoft.com/en-us/research/publication/foundations-and-trends-in-signal-processing-deep-learning-methods-and-aplications-now-publishers/

    Even though that progress of Neural Network into direction of structures with high number of [[glspl:hidden layer][hidden layer]] was obvious, its training was unsolved technical problem for very long time. There were basically 3 reasons why this breakthrough didn't come sooner.
    1. There were no efficient approach allowing number of [[glspl:hidden layer][hidden layer]] to scale.
    2. There wasn't enough of labeled data to effectively train the [[gls:nn][NN]].
    3. The computational hardware wasn't powerful enough to train sufficiently large and complex networks effectively.

    # TODO: add citation
    First problem was tackled by invention of [[glspl:cnn][CNN]] [citation].
    # LeCunn 1989
    Second problem was solved simply when there was more data available. This was mainly achieved thanks to effort of large companies (Google, Facebook, YouTube, etc.) but also with help of large community of professionals and hobbyists in data sciences.

    Both innovation in computational hardware and improvement of training methods were needed to solve the third problem. One of the technical breakthroughs was utilization of [[gls:gpgpu][GPGPU]]. Thanks to the fact that training process of [[glspl:nn][NN]] is typically large number of simple consequent computations, there is a great potential for parallelization (this is specifically true in case of [[glspl:cnn][CNN]]).

*** Model of neuron
    Typical structure of artificial neuron is shown on Fig. [[fig:neuron_model]].

     Neuron is computational unit that is
     # TODO: find better equation
     \begin{equation}
     h(x) = \boldsymbol{w}^T\boldsymbol{x} + b
     \end{equation}


     #+NAME: fig:neuron_model
     #+CAPTION: Model of artificial neuron used in Perceptron
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__neuron_model.png]]

***** Inputs
      Each neuron has multiple inputs that are combined together to execute some operation. Each input has designated weight assigned to it.

***** Weights
      Weights of a neuron are parameters that are modified during learning process. Each weight gives strength to each individual input into the neuron. The basic idea is that when the weight is small the particular input doesn't influence the output of the neuron very much and it has large influence in the opposite case.

***** Bias
      Bias is another modifiable parameter that controls influence of the neuron as a whole.

***** Activation Function
      [[Gls:af][AF]] of a neuron gives the [[gls:nn][NN]] the ability to approximate functions. In order to be effective learning algorithm it is necessary that the model is able to approximate the arbitrary function. For this reason the [[gls:af][AF]] has to be non-linear.

      # TODO: Update
      In the first iterations of [[gls:mlp][MLP]] evolution it generally had all neurons in hidden and output layer to be similar. Namely activation function was predominantly [[fig:sigmoid][sigmoid]].

      # TODO: remove
      # Main strength of [[gls:nn][NN]] is its ability to approximate the theoretically arbitrary nonlinear function. In order to do that it needs to introduce some sort of non-linearity.
      # Exactly for this reason is each neuron equipped with [[gls:af][AF]] on its output.

      # TODO: add some better equation describing activation function.
      \begin{equation}
      y=g(z) =g(\boldsymbol{w}^T\boldsymbol{x} + b)
      \end{equation}

      #+NAME: fig:relu
      #+CAPTION: Restricted Linear Unit (ReLU)
      #+ATTR_LATEX: :width 4in
      [[./img/figure__2__relu.png]]

      This function has to be nonlinear. Most commonly used [[gls:af][AF]] are [[gls:sigmoid][sigmoid]], softmax and [[gls:relu][relu]].

      # TODO: This doesn't fit here!!!
      Activation function of [[glspl:hidden layer][hidden layer]] is to this day one of the most dynamically evolving components of Neural networks. Currently there is several options but mainly used is Restricted Linear Unit (ReLU) [[fig:relu][relu]] which models following mathematical function

      \begin{equation}
      g(z) = \max \{0,z\}.
      \end{equation}

      #+NAME: fig:sigmoid
      #+CAPTION: Sigmoid activation function
      #+ATTR_LATEX: :width 4in
      [[./img/figure__2__sigmoid.png]]

      Model of neuron can have multiple different [[glspl:af][AFs]] on its output.

      Different options for [[glspl:af][AFs]] will be described in more detail in following sections.

      [[Gls:sigmoid][Sigmoid]] [[gls:af][AF]] has the advantage that is differentiable over all of its possible values. Another nice property is that its output value is between 1 and 0, which conveniently maps to valid probability distribution.
      Problem with sigmoid is that its gradient becomes really flat on both extremes and as such it slows down the learning process


*** Topology of the Network
    # TODO: feed-forward and recurrent below should be glossaries!!!
    There are several different commonly used topologies. Two most commonly used in [[gls:deep learning][deep learning]] are feed-forward and recurrent. Feed forward networks are characterized by the fact that during execution the information is flowing only in forward direction from inputs to output. The recurrent networks have some kind of feedback loop. Only feed-forward topology will be discussed further.

    Another criterion of topology is how are individual neurons in the network connected. Most commonly are [[glspl:nn][NN]] ordered in layers. In each layer there are multiple neurons and layer are hierarchically stacked. In typical terminology the first layer is called input layer, the last layer is called output layer and the layers in the middle are called hidden.

    Last aspect of topology are interconnections between individual layers and neurons within these layers. Most common scheme is called fully connected where each neuron in hidden layer $l$ has input connections from all neurons from previous layer $l-1$ and its output is connected to input of each neuron in following $l+1$ layer. Entire structure is depicted on Fig. [[fig:net_structure]].

    #+NAME: fig:net_structure
    #+CAPTION: Fully connected feed forward neural network
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__net_structure.png]]

    Neurons in individual layers are dependent on the type of layer. Currently the main difference is in their [[gls:af][af]], which wasn't the case for a long time. For a long time in the history were neurons in [[gls:nn][NN]] equipped with [[gls:sigmoid][sigmoid]] [[glspl:af][AF]]. The main reason being that it is easy to find derivative of [[gls:sigmoid][sigmoid]] function. In the latest years it was found that network composed of neurons with [[gls:sigmoid][sigmoid]] [[glspl:af][AFs]] are difficult to train, mainly because they have tendency to saturate. Solution to this problem are [[gls:relu][ReLU]] [[gls:af][AFs]]. This [[gls:af][AF]] is used in input and hidden layers.

    # TODO: Make sure that each neuron in output layer actually has softmax function!!!
    Neurons in output layer need output that can produce probability distribution that can be used to estimate the probability of individual classes. For this reason most commonly used [[gls:af][AF]] of output neuron is softmax.

    From this point on the term [[gls:nn][NN]] will refer to Feed-forward fully connected Neural Network.


*** Learning algorithm
    Learning of [[gls:nn][NN]] consist of evaluating the cost function for given input data and modifying parameters of the [[gls:nn][NN]] in order to decrees it. Learning algorithm is complex because the [[gls:nn][NN]] has many parameters and it isn't obvious how a change of parameter influences the value of cost function.

    Technique that is used to solve this problem is called [gls:bp][BP]]. With the knowledge of how the change of parameter affects the value of cost function the optimization procedure that is used to find best possible solution. Is called gradient based optimization. More specifically most commonly is used the [[gls:sgd][SGD]] method. Both [[gls:bp][BP]] algorithm and [[gls:sgd][SGD]] will be described further.


*** Gradient Descent Optimization
    # TODO: This needs hard editing
    Gradient is computed with respect to each input variable. and result of this operation is representing the direction of most steep increase in the output value. Therefore in the heart of every gradient based optimization is an element of applying change proportional to negative gradient of inputs.

    #+NAME: fig:gradient_descent_conture_plot
    #+CAPTION: Depiction of Gradient based optimazation of on the conture plot.
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__gradient_descent_conture_plot.png]]


*** Back-propagation

*** Regularization

**** Early stoppage

**** Dropout


*** Meta-parameters
**** Learning rate
**** Momentum


*** FCNN in image processing
    It was found that general [[gls:fcnn][FCNN]] is not ideal for image processing needs. Even small images typically represents enormous amount of inputs (i.e. image of the size $64 \times 64$ pixels represents 4096 inputs).

    Since each of these inputs has to be connected to all neurons in following layer and weight of each connection has to be memorized, this represents enormous amount of parameters.


    # TODO: this is not substantiated!!!
    # Moreover because during the learning process update of these weights is computed via matrix multiplication for larger images this can be unresolvable problem, which exacerbate with the number of [[glspl:hidden layer][hidden layer]].

    The structure of [[gls:fcnn][FCNN]] has another deficiency for image processing application, which is that it doesn't capture geometric properties of information from input image. In other words because individual layers are fully connected (each output in lower layer is connected to each input in higher layer) networks are not capturing any information about relation of position of individual inputs (image pixels) to each other.

    Third problem is that for higher depth of [[gls:fcnn][FCNN]] increases the likelihood of getting stuck in some local minima.

    All of these problems were solved by the specific type of [[gls:nn][NN]] model called [[glspl:cnn][CNN]] [citation]

    # TODO: Try to find palce for this if it is relevant
    # For example in case of CNNs there is almost no need to process input image before it is used to train the model. Hiearchical extraction of image features that is automatically created by CNN is very advantages in this case.
    # of the fundamental two-dimensional property of image data.


** Convolutional Neural Networks
   [[glspl:cnn][CNN]] are specialized type of [[glspl:nn][NN]] that was originally used in image processing applications. They are arguably most successful models in [[gls:ai][AI]] inspired in biology. Even though they were guided by many different fields, the core design principles were drawn from neuroscience. Since their success in image processing, they were also very successfully deployed in natural language and video processing applications.

   Aforementioned inspiration in biology was based on scientific work of David Hubel and Torsten Wiesel. Hubel and Wisel, who were neurophysiologist, investigated vision system of mammals from late 1950 for several years. In the experiment, that might be considered little gruesome for today's standards, they connected electrodes into brain of anesthetized cat and measured brain response to visual stimuli [Citation]. They discovered that reaction of neurons in visual cortex was triggered by very narrow line of light shined under specific angle on projection screen for cat to see. They determined that individual neurons from visual cortex are reacting only to very specific features of input image. Hubel and Wiesel were awarded the Nobel Prize in Physiology and Medicine in 1981 for their discovery and their finding inspired design of [[glspl:cnn][CNN]].

   There will be several suppositions made in order to simplify explanation of the concepts involved:
   - It will be presumed that convolutional layer is working with rectangular input data (e.g. images). Even though the Convolutional networks can be also trained to use 1-dimensional input (e.g. sound signal) or 3-dimensional (e.g. [[gls:ct][CT]] scans) etc.
   - The complexity of multiple-channel inputs (i.e. colored images) will be ignored.
   - Each layer requires rectangular input and produces rectangular output per one [[gls:kernel][kernel]].

*** Structure of CNN
    # TODO: Try to find place for this!!
    # Keeping up with concepts of Neuron and Topology is little more difficult in case of [[gls:cnn][CNN]]. First reason being that the structure of [[gls:cnn][CNN]] is composed of three different types of layers and the second is the fact that some of these layers atypicall and hard to describe by concept of neuron!

    Structure of Convolutional networks is typically composed of three different types of layers. Layer can be of Convolutional, Pooling and [[gls:fc][FC]] type. Each type of layer has different rules for forward and error backward signal propagation.

    There are no precise rules on how the structure of individual layers should be organized. What is typical is that the network has two parts. First part usually called feature extraction that is using combinations of convolutional and pooling layer. Second part called classification is using fully connected layers.

    #+NAME: fig:cnn_structure
    #+CAPTION: Structure of Convolutional Neural Network
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__cnn_structure.png]]

    # TODO: This probably should be deleted
    # Even though there is no strict rule enforcing this, it custom to Network layers can pretty much arbitrarily combine these three types of layers (with exception of Fully-Connected layers, which always have to come last).

**** Convolutional layer

     As the name suggests this layer employs convolution operation. Input into this layer is simply called input. Convolution operation is performed on input with specific filter, which is called [[gls:kernel][kernel]]. Output of convolution operation is typically called [[gls:feature map][feature map]].

     Input into Convolutional layer is either image (in case of first network layer) or [[gls:feature map][feature map]] from previous layer. [[Gls:kernel][kernel]] is typically of square shape and its width can range from 3 to N pixels (typically 3, 5 or 7). [[Gls:feature map][feature map]] is created by convolution of [[gls:kernel][kernel]] over each specified element of input. Convolution is described in more detail in section describing training of [[gls:cnn][CNN]].

     Depending on the size of [[gls:kernel][kernel]] and layer's padding preferences the process of convolution can produce [[gls:feature map][feature map]] of different size than input. When the size of output should be preserved it is necessary to employ [[gls:zero padding][zero padding]] on the edges of input. [[Gls:zero padding][zero padding]] in this case has to add necessary amount of zero elements around the edges of input. This amount is determined by
     \begin{equation}
     p = ((h - 1) / 2)
     \end{equation}

     where h is width of used [[gls:kernel][kernel]]. In opposite case the [[gls:feature map][feature map]] is reduced by the $2*p$. Decreasing of the [[gls:feature map][feature map]] can be in some cases desirable.

     #+NAME: fig:zero_padding
     #+CAPTION: A zero padded 4x4 matrix
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__zero_padding.png]]


     Reduction of [[gls:feature map][feature map]] can go even further in case of use of [[gls:stride][stride]]. Application of [[gls:stride][stride]] specifies by how many input points is traversed when moving to neighboring position in each step. When the [[gls:stride][stride]] is 1, [[gls:kernel][kernel]] is moved by 1 on each step and the resulting size of [[gls:feature map][feature map]] is not affected.

     Each Convolutional layer is typically composition of several different kernels. In other words output of this layer is tensor containing [[gls:feature map][feature map]] for each used [[gls:kernel][kernel]]. Each of these is designed to underline different features of input image. In the first layers these features are typically edges. In following layers the higher the layer the more complex features are captured.

     Each [[gls:kernel][kernel]] that is used is applied to all inputs of the image to produce one [[gls:feature map][feature map]] which basically means that neighboring layers are sharing the same weights. This might not be sufficient in some applications and therefore it is possible to use two other types of connections. [[Gls:locally connected][Locally connected]] which basically means that applied [[gls:kernel][kernel]] is of the same size as the input and [[gls:tiled convolution][tiled convolution]] which means alternation of more than one set of weights on entire input.

     [[Gls:tiled convolution][tiled convolution]] is interesting because with clever combination with [[gls:max-pooling][max-pooling]] explained bellow it allows to train specific feature from multiple angles (in other words invariant to rotation).

     Each convolutional layer has non-linearity on its output that is sometimes also called the [[gls:detector stage][detector stage]].

**** Pooling layer

     This layer typically (more details later) doesn't constitute any learning process but it is used to down-sample size of the input. The Principle is that input is divided into multiple not-overlapping rectangular elements and units within each element are used to create single unit of output. This decreases the size of output layer while preserving the most important information contained in input layer. In other words pooling layer compresses information contained within input.

     Type of operation that is performed on each element determines a type of pooling layer. This operation can be averaging over units within element, selecting maximal value from element or alternatively learned linear combination of units within element. Learned linear combination introduces form of learning into the pooling layer, but it is not very prevalent.

     Selecting of maximal value is most common type of pooling operation and in that case the layer is called [[gls:max-pooling][max-pooling]] accordingly. Positive effect of Max-pooling down-sampling is that extracted features that are learned in convolution are invariant to small shift of input. [[gls:max-pooling][max-pooling]] layer will be used to describe process of training of [[glspl:cnn][CNN]].

     As already mentioned another advantage of Max-pooling arises when combined with [[gls:tiled convolution][tiled convolution]]. To create simple detector that is invariant to rotation it possible to use 4 different kernels that are rotated by 90 degrees among each other and when the [[gls:tiled convolution][tiled convolution]] is used to tile them in groups of 4, the Max-pooling makes sure that resulted [[gls:feature map][feature map]] contains output from the [[gls:kernel][kernel]] with strongest signal (i.e. the one trained for that specific rotation of the feature).

**** Fully-Connected layer

     Fully-Connected layer is formed from classical neurons that can be found in [[gls:fcnn][FCNN]] and it is always located at the end of the layer stack. In other words it is never followed by another Convolutional layer. Depending on the size of whole [[gls:cnn][CNN]] it can have 1 to 3 [[gls:fc][FC]] layers (usually not more than that). Input of the first [[gls:fc][FC]] layer has inputs from all neurons from previous layer to all neurons of following layer (hence fully connected). All [[gls:fc][FC]] layers are together acting as [[gls:fcnn][FCNN]].

*** Training of CNN
    Training process of [[gls:cnn][CNN]] is analogues to [[gls:fcnn][FCNN]] in that both are using [[gls:forward-propagation][forward-propagation]] and [[gls:bp][BP]] phases.

    Situation with [[gls:cnn][CNN]] is more complicated because network is composed of different types of layers and therefore training must accommodate for variability between different layers and also the individual convolution layers are sharing weights across all neurons in each layer.

    # TODO: This needs substantial upgrade !!!
    First phase is the [[gls:forward-propagation][forward-propagation]], where the signal is propagated from inputs of the [[glspl:cnn][CNN]] to its output.
    # TODO: Error function should be probably be called Loss function or maybe Cost function.
    In the last layer the output is compared with desired value by [[gls:loss function][loss function]] and error is estimated.

    Secondly in [[gls:bp][BP]] phase the error is propagated backwards through the network and weights for individual layers are updated by its contribution on the error. Most commonly used algorithm for update of weights is [[gls:gradient descent][gradient descent]]. It is not the only one used but in majority of cases the training algorithm is at least based on [[gls:gradient descent][gradient descent]].

**** Forward Propagation of Convolution Layer
      # TODO: fix this sentence
      Each convolutional layer has inputs. In case that the layer is first, it is network input (i.e individual pixels of image) in other cases, the inputs are outputs from neurons from previous layer (this is typically pooling layer).

      Presuming that input of a layer is of size $N x N$ units and [[gls:kernel][kernel]] is of size $m x m$. Convolution is computed over $(N-m+1) x (N-m+1)$ units (presuming that there is no zero padding).

      Computation of convolution output $x_{ij}^{(l)}$ is defined as
      \begin{equation}
     x_{ij}^{(l)}=\sum_{a=0}^{m-1}\sum_{b=0}^{m-1}\omega_{ab}y_{(i+a)(j+b)}^{(l-1)}
      \end{equation}

 where $i, j \in (0,N-m+1)$, l is index of current layer, $\omega_{ab}$ are weights of layer ([[gls:kernel][kernel]]) and $y_{(i+a)(j+b)}^{(l-1)}$ is output of previous layer.

      Output of convolutional layer $y_{ij}^{(l)}$ is computed by squashing of output of convolution operation $x_{ij}^{(l)}$ through non-linearity:

      \begin{equation}
      y_{ij}^{(l)}=\sigma(x_{ij}^{(l)})
      \end{equation}
where $\sigma$ represents this non-linear function.
equation

**** Forward Propagation of Pooling layer (Max-Pooling)

   Feed forward operation of pooling layer is generally very simple and it constitutes in selecting of maximal value within subset
   pooling of multiple inputs into single output.
   Ratio is typically $4 to 1$, which means that input matrix is divided into not-overlapping sub-matrices of size $2 \times 2$ and each of these produces 1 output. Size of sub-matrices can vary and is dependent on size of input, number of layers.

**** Forward Propagation of Fully Connected layer

     Signal is distributed through [[gls:fc][FC]] layer in similar fashion as in Convolutional layer. The main difference is that weights of individual neuron connections are not shared among all neurons in one layer.

**** Backward Propagation of Convolution Layer
     # TOOD: Finish this!!
     # To estimate contribution of convolutional layer to the total error of CNN,
     # there needs to be computed gradient of error function
     Following equasions were lifted from \cite{book--goodfellow--2016}.

     \begin{equation}
     \frac{\partial E} {\partial \omega_{ab}}
     =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial E}{\partial x_{ij}^{l}} \frac{\partial x_{ij}^{l}} {\partial \omega_{ab}}
     =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial E}{\partial x_{ij}^{l}} y_{(i+a)(j+b)}^{l-1}
     \end{equation}

     \begin{equation}
     \frac{\partial E} {\partial x_{ij}^{(l)}}
     =\frac{\partial E} {\partial y_{ij}^{l}} \frac{\partial y_{ij}^{l}} {\partial x_{ij}^{l}}
     =\frac{\partial E} {\partial y_{ij}^{l}} \frac{\partial} {\partial x_{ij}^{l}} \left( \sigma\left(x_{ij}^{l}\right) \right)
     =\frac{\partial E} {\partial y_{ij}^{l}} \sigma' \left( x_{ij}^{l} \right)
     \end{equation}

     \begin{equation}
     \frac{\partial E} {\partial y_{ij}^{l-1}}
     =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial E} {\partial x_{(i-a)(j-b)}^{l}} \frac{\partial x_{(i-a)(j-b)}^{l}} {\partial  y_{ij}^{l-1}}
     =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial E} {\partial x_{(i-a)(j-b)}^{l}} \omega_{ab}
     \end{equation}

**** Backward Propagation of Pooling layer (Max-Pooling)
     As mentioned in section for [[gls:forward-propagation][forward-propagation]], there is no explicit learning process happening in pooling layer. Error is propagated backwards depending on how the signal was propagated forward. In case of [[gls:max-pooling][Max-Pooling]] layer the error is propagated only to the unit with maximal output in [[gls:forward-propagation][forward-propagation]] phase (in other words to the winner of pooling). The error is propagated very sparsely, as result.

     # TODO: Delete the bit about everage pooling it is not necessary!!!
     In case of different pooling method it is adjusted accordingly (i.e. for /average pooling/ the error is propagated according to contribution of individual neurons).

**** Backward Propagation of Fully connected layer
     Training mechanism for [[gls:fc][FC]] layer if following the same principles as in [[gls:fcnn][FCNN]], which is not a subject of detailed discussed here. It is similar to one for convolution layers and from our perspective is only important that the first (last in the sense of [[gls:bp][BP]]) [[gls:fc][FC]] layer propagates error gradient of each neuron in it, that is then send to all neurons in preceding (following in the direction of [[gls:bp][BP]]) layer.
*** Advantages of CNN
    # TODO: Find out what I meant by this!!
    # Number of parameters
    # computational demand
    To further highlight the difference between [[gls:fcnn][FCNN]] and [[gls:cnn][CNN]] it is worth to compare the case of 2 neighboring layers.
    Lets have gray scale input image of size 32x32 pixels and following layer will be convolutional with 6 feature maps of size 28x28. Kernels used in this convolutional layer will have the size of 5x5. In this case we have totally $(5 * 5 + 1) * 6 = 156$ parameters between the two layers.
    If we would like to create equivalent connection between two layers of [[glspl:fcnn][FCNN]], then it would have mean $(32 * 32 + 1) * 28 * 28 = 803600$ connections (parameters). Which means that difference between the two is of ~5000 ratio.
    This difference would rise exponentially with larger images or with more color channels. When input size of the image changes to 64x64 and it has [[gls:rgb][RGB]] color then [[glspl:fcnn][FCNN]] would requires $(64 * 64 * 3 + 1) * 28 * 28 = 9634576$ connections (parameters). In the same case the [[gls:cnn][CNN]] only needs $(5 * 5 * 3 + 1) * 6 = 456$ parameters. Which is difference of ~20000 factor.
    Just to elaborate, in case that [[gls:cnn][CNN]] would be used to process video. Analogically to previous examples in case of moving image in time the number of parameters raises linearly with number of images in analyzed video.

* Theory
  This chapter is focused on defining concepts of artificial intelligence, image processing and machine learning in general.

** Artificial Intelligence
   Field of [[gls:ai][AI]] is very general and spawns across several different disciplines (e.g. mathematics, computer science, philosophy, economics and even ethics). This fact suggests that this field is very broad and can be tackled from many different view points. Therefore, this description will not be very exhaustive. For broader and more complex dive into subject refer to \cite{book--russell--2003}.

   One of many possible definitions of [[gls:ai][AI]] can be summarized as a pursuit to develop an artificial intelligent agent. In other words, it is an attempt to create intelligent machines that are either thinking or can be perceived as thinking ones. One of the most important abilities of intelligent agents is the sense of vision[fn:1]. Sense of vision is usually required to certain degree and not always it is necessary that it rivals the capabilities of human visual apparatus.

   First attempts to solve the vision problems were tackled from so called bottom-up approach in which the system was instructed with hard-coded set of rules describing the vision. It was expected that as the understanding of a mechanisms allowing humans to extract information from visual scene, the hard-coded systems can be fed this understanding and thus more capable systems can be created. Problem with this approach was that it highly underestimated the difficulty of formalization of these rules. As a consequence it predominantly failed to devise such a system.

   This insight leads the researchers to postulating that in order to solve the problem of deploying vision capabilities for artificial intelligent system, it is necessary to introduce a process that would allow [[gls:ai][AI]] systems to extract patterns from provided data. That is to say, it is an introduction of systems that are able to learn. Process that enables systems to learn is generally called [[gls:machine learning][machine learning]].

   [[Gls:machine learning][Machine learning]] is again quite extensive term that can be used in multiple different contexts. In this work, it is meant to be understood as technique that is used to create mathematical models used for image recognition. There are several types of machine learning models that are useful for different tasks. The task that is discussed in this work and is also arguably most commonly tackled is called [[gls:classification][classification]], which is the task to classify instance of input into correct discreet and mainly finite class. Another common type of machine learning task is called [[gls:regression][regression]], which is based on the input data trying to estimate unknown continues-valued quantity.

   Rest of this document will be exclusively dealing with learning tasks of [[gls:classification][classification]] type.

# TODO: Make sure that numbering of each footnote is correct!!!
[fn:1] This is highly dependent on concrete application.


** TODO Image Processing

   # TODO: Add citation to MPOV slides
   Computer vision is important subject in image processing. Research in the subject was approaching the problem from bottom-up perspective for many years. This was the before-mentioned effort to formalize rules guiding the vision of living organisms. This approach was actually very successful in certain circumstances.

   General description of computer vision in image processing can be summarized in following steps:
    - Image capture - Image is captured (by camera or similar device) and digitized.
    - Pre-processing - Digitized image is modified to emphasize important features (e.g. noise reduction, contrast normalization etc.).
    - Segmentation - Selection of interesting features (edges, similar surfaces).
    - Description - Extraction of radiometric, photometric descriptors and so on.
    - Classification - Some means to classify the object.

    #+NAME: fig:classification_chain
    #+CAPTION: Block diagram of image processing pipeline. \cite{image__preprocesing}
    #+ATTR_LATEX: :width 5in
    [[./img/figure__2__classification_chain.png]]

    Individual steps are shown in figure [[fig:classification_chain]]. Even though [[gls:machine learning][machine learning]] existed as field of study since the second half of the 20th century, there was no wider adoption of its techniques in image processing for very long time. It was first introduced in the classification step of the processing pipeline. In other words, complex problems were simplified by reducing the visual information contained within the image into handful of simple features that were fed into machine learning model. This approach consequently favors very simple models such as [[gls:knn][KNN]] or [[gls:svm][SVM]].

    This brings the problem that these applications are not very versatile. Each application is usually only capable of solving very narrow problem and any deviation from ideal circumstances can mean failure. Application can have problems with varying contrast, illumination, scaling, rotation etc.

    Second problem is often the fact that because image has to be pre-processed several times before it is fed into machine learning model it requires extra time and computation resources. This is less of a problem with current hardware innovations, but it is still not negligible factor and it can have negative effect on the cost of the solution. This is where [[gls:machine learning][machine learning]] in general indicates significant advantage.

    Classical approach to computer vision can find success in applications that are deployed in very restricted environments with rigid constraints[fn:2]. In controlled environment, it is usually very simple to describe the problem in formal rules. Even though it can be still found in certain places, it starts to be forced out by application of machine learning simply because the barrier of entry for wider adoption decreases every day.

[fn:2] This could be for example detection of defects on line production in industrial automation.


** Machine Learning

   As previously described, Machine Learning is a process that is used to create models that are able to extract information from data to solve given problem and consequently automatically improves their performance.

   Interesting perspective that can be used is to view machine learning as form of information compression. Where machine learning model is trying to extract information from input data in such a way that the amount of a data used to save is reduced while the information contained within is preserved.

*** Machine Learning Approach
    There are typically two different types of machine learning approaches:
    - Unsupervised Learning
    - Supervised Learning

    Both are typically used for different kinds of machine learning tasks.

    # TODO : Maybe add reinforcement learning as well
**** Unsupervised learning
     In this learning approach, the model is training by observing new data and extracting patterns in the date without being instructed on what they are. Opposed to supervised learning described below, the advantage of this approach is that the model is able to learn from data without supervision (as the name suggests). This means that there is no need for input data to be annotated, therefore it takes much less time and resources to deploy these models in practice.

     The biggest hurdle of supervised learning approach in real world applications is to obtain appropriate data. Appropriate data in this context means, data that were somehow classified into different categories, which can be very tedious and slow process. In some cases, the task itself prevents the usage of labeled data (i.e. labeled data are impossible to obtain or don't exist at all).

     Majority of unsupervised learning algorithms belong to group called clustering algorithms. These algorithms are centered on the idea to analyze geometric clustering of data in input space to determine their affiliation. This is achieved by the presupposition that data point clustering in input space are likely to exhibit similar properties.

     Examples of unsupervised learning models are:
     - [[gls:k-means][K-MEANS]] - clustering model \cite[p.~460--462]{book--hastie--2008};
     - [[glspl:som][SOM]] - instance based \cite{book--kohonen--2001};
     - [[gls:pca][PCA]] - dimensionality reduction \cite[p.~534--544]{book--hastie--2008}.

     Image classification usually does not rely heavily on the use of unsupervised learning methods, therefore the following text describes only supervised learning methods.

**** Supervised learning
     Supervised learning approach is more commonly used. This approach requires training data with specific format. Each instance has to have assigned label. These labels provide supervision for the learning algorithm.
     Training process of supervised learning is based on the following principle. Firstly, the training data are fed into the model to produce prediction of output. This prediction is compared to the assigned label of the training data in order to estimate model error. Based on this error the learning algorithm adjusts model's parameters in order to reduce it.

*** Structure of Machine Learning Algorithm
    Although machine learning algorithms are diverse and are using different techniques its structure can be generalized. Structure of nearly all machine learning algorithms can be described as composition of the following components:
    - Dataset specification
    - Model
    - Cost function
    - Optimization procedure

    Almost all supervised learning algorithms use the same Dataset specification. The other three components can vary dramatically. This level of analysis is useful for building of intuition for [[glspl:nn][NN]] and explanation of its individual components.

**** Dataset specification
     Supervised learning requires datasets with specific properties. Each dataset contains set of $n$ instances which consists of a pair of input vector $\boldsymbol{x}_i$ and output scalar $y_i$. Input vector

     \begin{equation}
     \boldsymbol{x}_i^T = [x_1, x_2, \dotsc, x_p],
     \end{equation}
     where $i$ is index of instance, $p$, is dimension of input vector.

     Individual components of input vector have to be of unified type. In case of input data in form of image it is value for individual pixels (e.g. 0-255). In other cases, they can be real values. Almost universally in machine learning it stands that input should be normalized. This presumption holds in images automatically since each pixel has to have its vales in fixed range.
     It is very important in other types of machine learning tasks, where this is not guaranteed.

     Output scalar $y_i$ represents class of given instance. Type of this output value thus has to acquire only certain values. To put it differently, it has to be a set of cardinality equal to number of all possible classes.

**** Model
     Model is prediction apparatus that takes input $\boldsymbol{x}_i$ to predict value of it's output $y_i$. Each model has parameters represented by vector $\boldsymbol{\theta}$, which are adjusted during the training process. The simplest example of model type is linear model, also called [[gls:linear regression][linear regression]].

     Parameters $\boldsymbol{\theta}$ of this model are
     \begin{equation}
     \boldsymbol{\theta}^T = [\theta_1, \theta_2, \dotsc, \theta_p],
     \end{equation}
     where $p$ is number of parameters equal to size of input vector $\boldsymbol{x}_i$.

     Prediction $\hat{y}_i$ of the model on instance $i$ is computed as
     \begin{equation}
     \hat{y}_i =  \sum_{j=1}^{p} x_{ij} \theta_j.
     \end{equation}

     Therefore predictions of the model on the entire dataset in matrix notation is
     \begin{equation}
     \boldsymbol{\hat{y}} = \boldsymbol{X}\boldsymbol{\theta}.
     \end{equation}

     Predictions in expanded notation are equal to
     \begin{equation}
        \begin{bmatrix}
          \hat{y}_{1} \\
          \vdots      \\
          \hat{y}_{n}
        \end{bmatrix}
        =
        \begin{bmatrix}
          x_{11} & \cdots & x_{1p} \\
          \vdots & \ddots & \vdots \\
          x_{n1} & \cdots & x_{np}
        \end{bmatrix}
        \begin{bmatrix}
          \theta_{1} \\
          \vdots     \\
          \theta_{p}
        \end{bmatrix}.
     \end{equation}

**** Cost Function
     To achieve the learning ability of the machine learning algorithm, it is necessary to estimate the error of its predictions. This is estimated with so called [[gls:cost function][cost function]] (also sometimes called [[gls:loss function][loss function]]).

     This function has to have certain properties. Ability of the machine learning algorithm to learn rests on the estimation of its improvement with change of its parameters. Therefore, [[gls:cost function][cost function]] has to be at least partially differentiable. In case of [[gls:linear regression][linear regression]] it is most common to use [[gls:sum of square][sum of square]] error. The main reason being that derivative of this function for linear model has only one global minimum.

     [[Gls:cost function][Cost function]] is defined as
     \begin{equation}
     J(\boldsymbol{\theta}) = \sum_{i=1}^{n}{\left(y_i - \hat{y_i}\right)^2} =
     \sum_{i=0}^{n}{\left(y_i - \boldsymbol{x_i}^T \boldsymbol{\theta} \right)^2}.
     \end{equation}

     For the optimization purposes it is usually useful to express the [[gls:cost function][cost function]] in matrix notation
     \begin{equation} \label{eq:linear_cost}
     J(\boldsymbol{\theta}) = \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right)^T \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right).
     \end{equation}

**** Optimization Procedure
     The last part of learning algorithm is the optimization procedure. It consist of update of model's parameters $\boldsymbol{\theta}$ in order to improve it's prediction. In other words to find $\boldsymbol{\theta}$ such that the value of [[gls:cost function][cost function]] $J(\boldsymbol{\theta})$ for given dataset is as small as possible.

     To investigate the change of [[gls:cost function][cost function]] on given dataset it is necessary to compute the derivative of $J(\boldsymbol{\theta})$ with respect to $\boldsymbol{\theta}$
     \begin{equation}
      \begin{split}
        \frac{\partial J(\boldsymbol{\theta})} {\partial \boldsymbol{\theta}} & = \frac{\partial} {\partial \boldsymbol{\theta}} \left[ \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right)^T \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right) \right] \\
        & = \frac{\partial} {\partial \boldsymbol{\theta}} \left[ \boldsymbol{y}^T \boldsymbol{y} + \boldsymbol{\theta}^T \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\theta} - 2\boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\theta} \right] \\
        & = 2\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\theta} - 2\boldsymbol{X}^T\boldsymbol{y}.
      \end{split}
     \end{equation}

     For linear model is possible to find optimal solution which is global minimum of the [[gls:cost function][cost function]].
     The optimal solution
     \begin{equation}
      \boldsymbol{\theta} = \left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y},
     \end{equation}
     is found by equating the partial derivative of $J(\boldsymbol{\theta})$ to $0$. Only condition is that $\boldsymbol{X}^T\boldsymbol{X}$ has to be non singular.

     Unfortunately, only very simple problems can be approximated using model as simple as [[gls:linear regression][linear regression]]. More complex model usually means more complicated [[gls:cost function][cost function]]. Optimization process of more complex [[glspl:cost function][cost functions]] cannot be guaranteed to find global minimum. In this case, the optimization procedure has to be of iterative character. To put it in a different way, algorithm has to approach the minimum of iterations. Many of the iterative methods belong to the group called gradient based optimization.

*** TODO Model Complexity

    In the first approximation it could be said that the task of supervised machine learning is to model relationship between the input output data most accurately. The problem with this definition is that in the practical application there is never enough data to capture true relationship between the two. Therefore, the task of machine learning is the attempt to infer true relationship by observing incomplete picture.

    Hence the most important property of machine learning model is its generalization ability. That is ability to produce meaningful results from data that were not previously observed.

    #+NAME: fig:over_under_fitting
    #+CAPTION: Figure shows different levels of generalization of model \cite{image__over_under_fitting}
    [[./img/figure__2__over_under_fitting.png]]

    Generalization ability is dependent on complexity of the model and its relationship to complexity of underling problem. When model does not capture complexity of the problem sufficiently it is described as [[gls:under fitting][under fitting]]. In case the complexity of model exceeds the complexity of underling problem then this phenomenon is called [[gls:over fitting][over fitting]].

    In both of these extremes the generalization ability suffers. In the former case the model is unable to capture true intricacies of the problem and therefore is unable to predict desired output reliably. In the latter case it tries to capture even the subtlest data perturbation that might be in fact a result of stochastic nature of the problem and not the real underlying relationship. This can also cause the fact that input data is missing some variable necessary to capture the true relationship. This fact is unavoidable and it thus has to be taken into account when designing machine learning model. Depiction of this phenomena in case of two variable inputs is on Figure [[fig:over_under_fitting]].

    Typically, the machine learning model is trained on as much input data as possible in order to achieve the best possible performance. At the same time its error rate has to be verified on independent input data to check whether the generalization ability is not deteriorating. This is typically achieved by splitting available input data into training and testing set (usually in 4:1 ratio for training to test data). Model is trained with training data only and the performance of the model is tested on the test data. Relationship between test and train error can be found on Fig. [[fig:test_vs_training_error]]. Even though the true generalization error can never be truly observed, its approximation by test error rate is sufficient for majority of machine learning tasks.

    #+NAME: fig:test_vs_training_error
    #+CAPTION: Relationship between the model complexity and its ultimate accuracy is the relationship between training and testing error \cite{image__test_vs_train_error}.
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__test_vs_training_error.png]]


**** Regularization
     /Regularization is any modification that is made to the learning algorithm that is intended to reduce its generalization error but not its training error/ \cite{book--goodfellow--2016}.

     As it has already been mentioned, the most important aspect of machine learning is striking the balance between over and under fitting of the model. To help with this problem concept of regularization was devised. It is a technique that helps to penalize the model for its complexity. Basic concept consists of adding a term in the [[gls:cost function][cost function]] that increases with model complexity.

     # TODO: I think that this should be called L2 regularization
     When this is applied to cost function from equation \ref{eq:linear_cost}
     \begin{equation}
     J(\boldsymbol{\theta}) = \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right)^T \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right) + \lambda\boldsymbol{\theta}^T\boldsymbol{\theta},
     \end{equation}
     where $\lambda$ is a parameter that controls the strength of the preference \cite{book--goodfellow--2016}.

     # TODO: TBD

* Theory
  In this chapter will discussed concepts of Artificial intelligence, Image processing, Machine learning in general, Neural networks and finally the Convolutional neural networks.

** Artificial Intelligence
   Field of [[gls:ai][AI]] is very general and spawns across several different disciplines (e.g. computer science, mathematics, philosophy, economics and even ethics). This suggests that this field is very broad and as such it won't be attempted in this document to be exhaustive or in no way complete.

   It could be characterized by a pursuit of construction of intelligent agents. In other words attempt to create intelligent machines are either thinking or can be perceived as thinking ones.

   One of the most important abilities of intelligent agents is the sens of vision (even thought this is highly dependent on concrete application). Sense of vision is usually required to certain degree and not always it is necessary that it rivals the capabilities of for example human visual apparatus.

   First attempts to solve the vision problems were tackled from so called bottom up approach where system was instructed with hard-coded set of rules that were describing rules of vision. It was expected that as the understanding of mechanisms that allows humans to extract information from visual scene then these hard-coded systems can be fed more information in order to create more capable systems. Problem with this approach was that it highly underestimated the difficulty of formalization of these rules that could be prescribed into the intelligent system.

   This insight lead the researchers to postulating that in order to solve this problem there needs to be introduction of process that would allow the [[gls:ai][AI]] systems to extract patterns from provided data. In other words introduction of systems that are able to learn. Process that enables systems to learn is generally called /machine learning/.

   There are several types of machine learning tasks. Most commonly encountered task is called /classification/. is the task to classify instance of input into correct discreet and predominantly finite class. Another typical type of machine learning task is called regression, which is based on the input data trying to estimate value of unknown quantity. Usually integer or real type.

   Rest of this publication it will be exclusively dealing with /classification/ learning tasks.

   # Basics of image processing techniques.
** TODO Image Processing

   # There was no wider adoption of machine learning techniques in image processing for very long time, even though they existed as field of study since 1950's. Reason being that machine learning algorithms were very simple and therefore unfit for generally very complex problems of image processing (e.g. object detection and classification).

   Research in image processing was adopting predominately bottom up (feature-based) methodologies. Typical classification pipeline in computer vision would be following:
   - Image capture - Image is captured (by camera or similar device) and digitized.
   - Preprocessing - Digitized image is modified to emphasize important features (e.g. noise reduction, contrast normalization etc.).
   - Segmentation - Selection of interesting features (edges, similar surfaces).
   - Description - Extraction of radiometric, photometric descriptors and so on.
   - Classification - Some means to classify the object.


   # Classical approach to image processing is still very useful in very restricted environments with rigid constraints. One of examples can be detection of defects on line production in industrial automation.

   In the first attempts to apply machine learning in image processing, it was used usually deployed as the classification model. In other words complex problems were reduced and simplified in order to utilize machine learning techniques. Consequently this approach favors simpler models.

   This also brings the problem that any of these applications is not very versatile. Each application is usually only capable of solving very simple problem and any deviation from ideal circumstances can mean failure. Application can have problem with varying contrast, illumination, scaling, rotation etc.

   Second problem is often the fact that because image has to be preprocessed several times before it is fed into machine learning model it demands extra time and resources. This can created problems in training phase but also during operation after deployment.

   This is less of a problem with technical innovation and today it can be solved by very fast [[gls:dsp][DSP]], but it still is not negligible effect and it can have negative affect on the price of the solution. This is where Deep learning models starts to exhibit significant advantage.

   Deep Learning models are in theory capable of capturing complex (higher order) features of the input due to the fact they can learn very high level of abstraction from even very low level features. And what is even more important, is the fact that Deep learning model in theory doesn't need any kind (or very little) of preprocessing. Input image can be directly connected as input into Deep Network.

   # Basics of Machine learning
** Machine Learning
   # REMOVE:
   # It is necessary to properly define machine learning before we move forward. There are many valid definitions that may differ but in this text we will adhere  to the following.
   # Field of Machine learning is a subset of [[gls:ai][AI]] that is describing tools that can be used to solve problems.

   As previously described Machine learning is a process that is used to create models that are able to extract information from data to solve given problem and consequently automatically improve their performance. There are typically two different types of machine learning approaches:
   - Unsupervised Learning
   - Supervised Learning

   # ??? Maybe find better word for this
   Both of these are typically used for different kinds of machine learning tasks.

*** TODO Unsupervised learning
    In this learning approach the model is training by observing new data and extracting patterns in the date without being instructed of what they are. Interesting perspective that can be used to view this approach is to take unsupervised learning as form of information compression. Where model is trying to extract data from input in such a way that reduces the input while preserving the information contained within.

    # this and the pharagraf bellow sais the same thing !!!
    Opposed to supervised learning that is described bellow the advantage of this approach that model is able to learn from data, as the name suggests, without supervision. This means that  there is no need for input data to be annotated, therefore it takes much less time and resources to deploy this model in practice.

    For real life task with supervised learning approach the biggest hurdle is to find appropriate data that could be used. Appropriate data in this context mean, data that were somehow classified into different categories, which can be very tedious and slow process.

    There is several different approaches to unsupervised learning. As main example there is [[gls:pca][PCA]], that is mainly used in statistical analysis of data. Another very important group of unsupervised learning models are clustering models, which are analyzing clustering of values in the input space and based on this information it assigns output values that are likely to represent similar properties. One of the models that is used often is called /k-means/ [citation]. Another model that is representative of this group that is at the same time member of family of neural networks and was already mentioned is called [[glspl:som][SOM]].

    # In following text we will only deal with supervised learning.

*** TODO Supervised learning
    Supervised learning approach is most commonly used for the classification tasks. As was already mentioned this approach requires training data to be in specific format. Namely each instance has to have a class that corresponds to a type of the instance.

    In the first approximation it could be said that the task of supervised machine learning is to model relationship between the input output data most accurately. The problem with this is that in the real world problems there is never enough data to capture true relationship between the two. Therefore the true task of machine learning is the attempt to infer true relationship by observing incomplete picture.

    Therefore the most important property of machine learning model is its generalization ability. That is ability to produce meaningful results on data that were not previously observed.

    #+NAME: fig:over_under_fitting
    #+CAPTION: Figure shows different levels of generalization of model
    [[./img/figure__2__over_under_fitting.png]]

    Generalization ability is dependent on complexity of the model and its relationship to complexity of underling problem. When model doesn't capture complexity of the problem sufficiently it is described as under fitting. In cases that the complexity of model is exceeds the complexity of underling problem then this phenomenon is called over fitting.

    In both of these extremes the generalization ability suffers. In the former case the model is unable to capture true intricacies of the problem and therefore is unable to reliably predict desired output. In the latter case it tries to capture even the most subtle data perturbation that might be in fact a result of stochastic nature of the problem and not the real underlying relationship. This can be caused the fact that input data is missing some variable that is necessary to capture the true relationship. This fact is unavoidable and it therefore has to be taken into account when designing machine learning model. Depiction of theses phenomena in case of two variable input is on [[fig:over_under_fitting]].

    #+NAME: fig:model_complexity
    #+CAPTION: Figure shows relationship between model complexity and resulting error.
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__model_complexity.png]]

    Typically the machine learning model is trained on as much of input data as possible in order to achieve the best performance possible. At the same time its error rate has to be verified on independent input data to check whether the generalization ability is not deteriorating. This is typically achieved by splitting available input data into training and testing set (usually in 4:1 ratio for training to test data). Model is trained with training data only and the performance of the model is tested on the test data. Relationship between test and train error can be found on figure [[fig:test_vs_training_error]]. Even though that the true generalization error can never be truly observed its approximation by test error rate is sufficient for majority of machine learning tasks.

    #+NAME: fig:test_vs_training_error
    #+CAPTION: Different approach that examines the relationship between the model complexity and its ultimate accuracy is the relationship between training and testing error.
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__test_vs_training_error.png]]

    # We will use a definition from \cite{book--goodfellow--2016}

*** TODO Structure of machine learning algorithm
    #     Machine learning approach to solving problems can be generalized.
    Nearly all machine learning algorithms can be described using following components:
    - Dataset
    - Model
    - Cost function
    - Optimization procedure

    # Analyze of this Description of the learning algorithm by this way

    These components will be explained on /linear regression/ model due to its simplicity.

**** Dataset
     Supervised learning requires datasets with specific properties. Each dataset contains set of $n$ instances which consists of a pair of input vector $\boldsymbol{x}_i$ and output scalar $y_i$. Input vector

     \begin{equation}
     \boldsymbol{x}_i^T = [x_1, x_2, \dotsc, x_p],
     \end{equation}
     where $i$ is index of instance, $p$, is dimension of input vector.

     Output scalar $y_i$ represents class of given instance.

**** Model
     Model is prediction apparatus that takes input $\boldsymbol{x}_i$ to predict value of it's output $y_i$. Each model has parameters represented by vector $\boldsymbol{\theta}$, which are adjusted during the training process. One of the simplest examples of this model is linear model also called /linear regression/.

     Parameters $\boldsymbol{\theta}$ of this model are:

     \begin{equation}
     \boldsymbol{\theta} = [\theta_1, \theta_2, \dotsc, \theta_p],
     \end{equation}
     where $p$ is number of parameters equal to size of input vector.

     Prediction $\hat{y}_i$ of the model on instance $i$ is:
     \begin{equation}
     \hat{y}_i =  \sum_{j=1}^{p} x_{ij} \theta_j,
     \end{equation}

     Therefore predictions of the model on the entire datasets in matrix notation is:
     \begin{equation}
     \boldsymbol{\hat{y}} = \boldsymbol{X}\boldsymbol{\theta}
     \end{equation}

     That is,
     \begin{equation}
        \begin{bmatrix}
          \hat{y}_{1} \\
          \vdots      \\
          \hat{y}_{n}
        \end{bmatrix}
        =
        \begin{bmatrix}
          x_{11} & \cdots & x_{1p} \\
          \vdots & \ddots & \vdots \\
          x_{n1} & \cdots & x_{np}
        \end{bmatrix}
        \begin{bmatrix}
          \theta_{1} \\
          \vdots     \\
          \theta_{p}
        \end{bmatrix}.
     \end{equation}

     # It most general case it can be viewed as model that is generating probability distribution.
     # \begin{equation}
     # p(y \mid \boldsymbol{x}; \boldsymbol{\theta})
     # \end{equation}

**** Cost function
     In order to achieve the learning ability of the machine learning model it is necessary to estimate how correct the model is with its prediction. This is estimated with so called /cost function/ (also called /loss function/).

     # This function has to meet certain conditions. It has to represent a performance measure of the model.
     For the case of linear regression it is most common to use /Mean square error/.
     \begin{equation}
     J(\boldsymbol{\theta}) = \sum_{i=1}^{n}{\left(y_i - \hat{y_i}\right)^2} =
     \sum_{i=0}^{n}{\left(y_i - \boldsymbol{x_i}^T \boldsymbol{\theta} \right)^2}
     \end{equation}

     In matrix notation
     \begin{equation}
     J(\boldsymbol{\theta}) = \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right)^T \left(\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\theta}\right)
     \end{equation}

     # relationship between the correct
     # Firstly it has to be differentiable.

**** Optimization procedure
     The last part of learning algorithm is the optimization procedure. It consist in update of model's parameters $\boldsymbol{\theta}$ in order to improve it's prediction. In other words to find $\boldsymbol{\theta}$ such that the value of /cost function/ $J(\boldsymbol{\theta})$ for given dataset is as small as possible.

     To investigate the change of /cost function/ on given dataset it is necessary to compute the derivative of $J(\boldsymbol{\theta})$ in respect to $\boldsymbol{\theta}$
     \begin{equation}
     \frac{\partial J(\boldsymbol{\theta})} {\partial \boldsymbol{\theta}} =
     \frac{\partial} {\partial \boldsymbol{\theta}} \left[ \boldsymbol{y}^T \boldsymbol{y} + \boldsymbol{\theta}^T \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\theta} - 2\boldsymbol{y}^T\boldsymbol{X}\boldsymbol{\theta} \right]
     \end{equation}


** Neural Networks
  Neural networks are definitely inspired by biology. First attempts to created model of neuron had multiple elements equivalent with neurons of human brain. As time progressed this equivalence sized to being as important and modern neural networks models correspond to h their biological counterparts only superficially.

*** Computational model
In this case the computational model is composed of a network of nonlinear units that are processing input in several

taking input and applying weights to it in
*** Cost (loss) function
*** Optimization algorithm
*** Nonlinear models

    Problem with nonlinear models is that their optimization doesn't result in quadratic optimization there fore it is not possible to guarantee the finding of optimal solution. It is necessary to use iterative approach to solving the optimization problem

 they require stochastic

*** Activation functions
    In the first iterations of [[gls:mlp][MLP]] evolution it generally had all neurons in hidden and output layer to be similar. Namely activation function was predominantly sigmoid.

    #+NAME: fig:sigmoid
    #+CAPTION: Sigmoid activation function
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__sigmoid.png]]

    Activation function of hidden layers is to this day one of the most dynamically evolving components of Neural networks. Currently there is several options for but mainly used is Restricted Linear Unit (ReLU) which models following mathematical function

    \begin{equation}
    g(z) = \max \{0,z\}
    \end{equation}

    #+NAME: fig:relu
    #+CAPTION: Restricted Linear Unit (ReLU)
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__relu.png]]

*** Gradient Descent Optimization
    Gradient is computed with respect to each input variable. and result of this operation is representing the direction of most steep increase in the output value. There fore in the heart of every gradient based optimization is an element of applying change proportional to negative gradient of inputs.

    #+NAME: fig:gradient_descent_conture_plot
    #+CAPTION: Depiction of Gradient based optimazation of on the conture plot.
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__gradient_descent_conture_plot.png]]

*** Stochastic Gradient Descent
*** Network Architecture
*** Meta-parameters
**** Learning rate
**** Momentum

*** Deep Learning in image processing
    It was found that general *Fully Connected Neural Network* is not ideal for image processing needs. Even small images typically represents enormous amount of inputs (i.e. image of the size 64x64 pixels represents 4096 inputs).
    Since each of these inputs has to be connected to all neurons in following layer and weight of each connection has to be memorized, this represents enormous amount of parameters. Moreover because during the learning process update of these weights is computed via matrix multiplication for larger images this can be unresolvable problem, which exacerbate with the number of hidden layers.

    The structure of FCNN has another deficiency for image processing application, which is that it doesn't capture geometric properties of information from input image. In other words because individual layers are fully connected (each output in lower layer is connected to each input in higher layer) networks are not capturing any information about relation of position of individual inputs (image pixels) to each other.

    Third problem is that for higher depth of [[gls:fcnn][FCNN]] increases the likelihood of getting stuck in some local minima.

    All of these problems were solved by the specific type of [[gls:nn][NN]] model called [[glspl:cnn][CNN]] [citation]
    # For example in case of CNNs there is almost no need to process input image before it is used to train the model. Hiearchical extraction of image features that is automatically created by CNN is very advantages in this case.
    # of the fundamental two-dimensional property of image data.

# Basics of Neural Networks
** Convolutional Neural Networks
   [[glspl:cnn][CNN]] are specialized type of [[glspl:nn][NN]] that was originally used in image processing applications. They are arguably most successful models in [[gls:ai][AI]] inspired in biology. Even though they were guided by many different fields, the core design principles were drawn from neuroscience. Since their success in image processing, they were also very successfully deployed in natural language and video processing applications.

   Aforementioned inspiration in biology was based on scientific work of David Hubel and Torsten Wiesel. Hubel and Wisel, who were neurophysiologist, investigated vision system of mammals from late 1950 for several years. In the experiment, that might be considered little gruesome for today's standards, they connected electrodes into brain of anesthetized cat and measured brain response to visual stimuli [Citation]. They discovered that reaction of neurons in visual cortex was triggered by very narrow line of light shined under specific angle on projection screen for cat to see. They determined that individual neurons from visual cortex are reacting only to very specific features of input image. Hubel and Wiesel were awarded the Nobel Prize in Physiology and Medicine in 1981 for their discovery and their finding inspired design of [[glspl:cnn][CNN]].

   There will be several suppositions made in order to simplify explanation of the concepts involved:
   - It will be presumed that convolutional layer is working with rectangular input data (e.g. images). Even though the Convolutional networks can be also trained to use 1-dimensional input (e.g. sound signal) or 3-dimensional (e.g. [[gls:mri][MRI]] images) etc.
   - The complexity of multiple-channel inputs (i.e. colored images) will be ignored.
   - Each layer requires rectangular input and produces rectangular output per one kernel.

*** Structure of CNN

    Structure of Convolutional networks is typically composed of three different types of layers. Layer can be of Convolutional, Pooling and /Fully-connected/ type. Each type of layer has different rules for forward and error backward signal propagation.
    # Even though there is no strict rule enforcing this, it custom to Network layers can pretty much arbitrarily combine these three types of layers (with exception of Fully-Connected layers, which always have to come last).

**** Convolutional layer

     As the name suggests this layer employs convolution operation. Input into this layer is simply called input. Convolution operation is performed on input with specific filter, which is called kernel. Output of convolution operation is typically called /feature map/.

     Input into Convolutional layer is either image (in case of first network layer) or /feature map/ from previous layer. Kernel is typically of square shape and its width can range from 3 to N pixels (typically 3, 5 or 7). /Feature map/ is created by convolution of kernel over each specified element of input. Convolution is described in more detail in section describing training of CNN.

     Depending on the size of kernel and layer's padding preferences the process of convolution can produce /feature map/ of different size than input. When the size of output should be preserved it is necessary to employ /zero padding/ on the edges of input. /Zero padding/ in this case has to add necessary amount of zero elements around the edges of input. This amount is determined by
     \begin{equation}
     p = ((h - 1) / 2)
     \end{equation}

     where h is width of used kernel. In opposite case the /feature map/ is reduced by the $2*p$. Decreasing of the /feature map/ can be in some cases desirable.

     #+NAME: fig:zero_padding
     #+CAPTION: A zero padded 4x4 matrix
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__zero_padding.png]]


     Reduction of /feature map/ can go even further in case of use of stride. Application of stride specifies by how many input points is traversed when moving to neighboring position in each step. When the stride is 1, kernel is moved by 1 on each step and the resulting size of /feature map/ is not affected.

     Each Convolutional layer is typically composition of several different kernels. In other words output of this layer is tensor containing /feature map/ for each used kernel. Each of these is designed to underline different features of input image. In the first layers these features are typically edges. In following layers the higher the layer the more complex features are captured.

     Each kernel that is used is applied to all inputs of the image to produce one /feature map/ which basically means that neighboring layers are sharing the same weights. This might not be sufficient in some applications and therefore it is possible to use two other types of connections. /Locally connected/ which basically means that applied kernel is of the same size as the input and /tiled convolution/ which means alternation of more than one set of weights on entire input.

     /Tiled convolution/ is interesting because with clever combination with /max-pooling/ explained bellow it allows to train specific feature from multiple angles (in other words invariant to rotation).

     Each convolutional layer has non-linearity on its output that is sometimes also called the /detector stage/.

**** Pooling layer

     This layer typically (more details later) doesn't constitute any learning process but it is used to down-sample size of the input. The Principle is that input is divided into multiple not-overlapping rectangular elements and units within each element are used to create single unit of output. This decreases the size of output layer while preserving the most important information contained in input layer. In other words pooling layer compresses information contained within input.

     Type of operation that is performed on each element determines a type of pooling layer. This operation can be averaging over units within element, selecting maximal value from element or alternatively learned linear combination of units within element. Learned linear combination introduces form of learning into the pooling layer, but it is not very prevalent.

     Selecting of maximal value is most common type of pooling operation and in that case the layer is called /Max-Pooling/ accordingly. Positive effect of Max-pooling down-sampling is that extracted features that are learned in convolution are invariant to small shift of input. /Max-Pooling/ layer will be used to describe process of training of [[glspl:cnn][CNN]].

     As already mentioned another advantage of Max-pooling arises when combined with /Tiled convolution/. To create simple detector that is invariant to rotation it possible to use 4 different kernels that are rotated by 90 degrees among each other and when the /tiled convolution/ is used to tile them in groups of 4, the Max-pooling makes sure that resulted /feature map/ contains output from the kernel with strongest signal (i.e. the one trained for that specific rotation of the feature).

**** Fully-Connected layer

     Fully-Connected layer is formed from classical neurons that can be found in [[gls:fcnn][FCNN]] and it is always located at the end of the layer stack. In other words it is never followed by another Convolutional layer. Depending on the size of whole [[gls:cnn][CNN]] it can have 1 to 3 /fully connected/ layers (usually not more than that). Input of the first FC layer has inputs from all neurons from previous layer to all neurons of following layer (hence fully connected). All fully connected layers are together acting as [[gls:fcnn][FCNN]].

*** Training of CNN
    Training process of [[gls:cnn][CNN]] is analogues to [[gls:fcnn][FCNN]] in that both are using /Forward Propagation/ and /Backward Propagation/ phases.

    Situation with [[gls:cnn][CNN]] is more complicated because network is composed of different types of layers and therefore training must accommodate for variability between different layers and also the individual convolution layers are sharing weights across all neurons in each layer.

    First phase is the /Forward Propagation/, where the signal is propagated from inputs of the [[glspl:cnn][CNN]] to its output.
    # Error function should be probably be called Loss function or maybe Cost function.
    In the last layer the output is compared with desired values by /Error function E/ and error is estimated.

    Secondly in /Backward Propagation/ phase the error is propagated backwards through the network and weights for individual layers are updated by its contribution on the error. Most commonly used algorithm for update of weights is /Gradient Descent/. It is not the only one used but in majority of cases the training algorithm is at least based on /Gradient descent/.

**** Forward Propagation
***** Convolution Layer
      # fix this sentence
      Each convolutional layer has inputs. In case that the layer is first, it is network input (i.e individual pixels of image) in other cases, the inputs are outputs from neurons from previous layer (this is typically pooling layer).

      Presuming that input of a layer is of size $N x N$ units and kernel is of size $m x m$. Convolution is computed over $(N-m+1) x (N-m+1)$ units (presuming that there is no zero padding).

      Computation of convolution output $x_{ij}^{(l)}$ is defined as
      \begin{equation}
     x_{ij}^{(l)}=\sum_{a=0}^{m-1}\sum_{b=0}^{m-1}\omega_{ab}y_{(i+a)(j+b)}^{(l-1)}
      \end{equation}

 where $i, j \in (0,N-m+1)$, l is index of current layer, $\omega_{ab}$ are weights of layer (kernel) and $y_{(i+a)(j+b)}^{(l-1)}$ is output of previous layer.

      Output of convolutional layer $y_{ij}^{(l)}$ is computed by squashing of output of convolution operation $x_{ij}^{(l)}$ through non-linearity:

      \begin{equation}
      y_{ij}^{(l)}=\sigma(x_{ij}^{(l)})
      \end{equation}
where $\sigma$ represents this non-linear function.
equation

***** Pooling layer (Max-Pooling)

      Feed forward operation of pooling layer is generally very simple and it constitutes in selecting of maximal value within subset
      pooling of multiple inputs into single output.
      Ratio is typically $4 to 1$, which means that input matrix is divided into not-overlapping sub-matrices of size $2x2$ and each of these produces 1 output. Size of sub-matrices can vary and is dependent on size of input, number of layers.

***** Fully Connected layer

      Signal is distributed through FC layer in similar fashion as in Convolutional layer. The main difference is that weights of individual neuron connections are not shared among all neurons in one layer.

**** Backward Propagation
***** Convolution Layer
      # To estimate contribution of convolutional layer to the total error of CNN,
      # there needs to be computed gradient of error function
      Following equasions were lifted from \cite{book--goodfellow--2016}.

      \begin{equation}
      \frac{\partial E} {\partial \omega_{ab}}
      =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial E}{\partial x_{ij}^{l}} \frac{\partial x_{ij}^{l}} {\partial \omega_{ab}}
      =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial E}{\partial x_{ij}^{l}} y_{(i+a)(j+b)}^{l-1}
      \end{equation}

      \begin{equation}
      \frac{\partial E} {\partial x_{ij}^{(l)}}
      =\frac{\partial E} {\partial y_{ij}^{l}} \frac{\partial y_{ij}^{l}} {\partial x_{ij}^{l}}
      =\frac{\partial E} {\partial y_{ij}^{l}} \frac{\partial} {\partial x_{ij}^{l}} \left( \sigma\left(x_{ij}^{l}\right) \right)
      =\frac{\partial E} {\partial y_{ij}^{l}} \sigma' \left( x_{ij}^{l} \right)
      \end{equation}

      \begin{equation}
      \frac{\partial E} {\partial y_{ij}^{l-1}}
      =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial E} {\partial x_{(i-a)(j-b)}^{l}} \frac{\partial x_{(i-a)(j-b)}^{l}} {\partial  y_{ij}^{l-1}}
      =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial E} {\partial x_{(i-a)(j-b)}^{l}} \omega_{ab}
      \end{equation}

***** Pooling layer (Max-Pooling)
      As mentioned in section for /forward propagation/, there is no explicit learning process happening in pooling layer. Error is propagated backwards depending on how the signal was propagated forward. In case of /Max-pooling/ layer the error is propagated only to the unit with maximal output in /forward propagation/ phase (in other words to the winner of pooling). The error is propagated very sparsely, as result.

      In case of different pooling method it is adjusted accordingly (i.e. for /average pooling/ the error is propagated according to contribution of individual neurons).

***** Fully connected layer
      Training mechanism for FC layer if following the same principles as in FCNN, which is not a subject of detailed discussed here. It is similar to one for convolution layers and from our perspective is only important that the first (last in the sense of /Backward Propagation/) FC layer propagates error gradient of each neuron in it, that is then send to all neurons in preceding (following in the sense of /Backward Propagation/) layer.
*** Advantages of CNN
    # Number of parameters
    # computational demand
    To further highlight the difference between [[gls:fcnn][FCNN]] and [[gls:cnn][CNN]] it is worth to compare the case of 2 neighboring layers.
    Lets have gray scale input image of size 32x32 pixels and following layer will be convolutional with 6 feature maps of size 28x28. Kernels used in this convolutional layer will have the size of 5x5. In this case we have totally $(5 * 5 + 1) * 6 = 156$ parameters between the two layers.
    If we would like to create equivalent connection between two layers of [[glspl:fcnn][FCNN]], then it would have mean $(32 * 32 + 1) * 28 * 28 = 803600$ connections (parameters). Which means that difference between the two is of ~5000 ratio.
    This difference would rise exponentially with larger images or with more color channels. When input size of the image changes to 64x64 and it has [[gls:rgb][RGB]] color then [[glspl:fcnn][FCNN]] would requires $(64 * 64 * 3 + 1) * 28 * 28 = 9634576$ connections (parameters). In the same case the [[gls:cnn][CNN]] only needs $(5 * 5 * 3 + 1) * 6 = 456$ parameters. Which is difference of ~20000 factor.
    Just to elaborate, in case that [[gls:cnn][CNN]] would be used to process video. Analogically to previous examples in case of moving image in time the number of parameters raises linearly with number of images in analyzed video.

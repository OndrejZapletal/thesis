
* Neural Networks
  [[Gls:nn][NN]] is very general term describing broad family of models, due to a fact that its history goes back over 75 years. This situation often leads to confusion of some concepts. [[Gls:nn][NN]] is distributed and parallel model that is capable to approximate complex non-linear functions. Network is composed multiple computational units called neurons. Most commonly the network is composed of layers.

  Before the description of the model a brief history of [[glspl:nn][NN]] will be laid out in order to bring some terms into a context.

** History

   # TODO: Try to find place for this or delete!!!
   # Perceptron model in [picture citation] has multiple inputs $x_1,...,x_n$ that are weighted $w_1,...,w_n$ and summed together. Additionally each neuron has bias $b$ that controls its weight.
   # Result is squashed through nonlinear activation function to produce output. Depending on the application, output is bounded between values 0 to 1 or -1 to 1.

   # TODO: assign citation
   # (citation) http://web.csulb.edu/~cwallis/artificialn/History.htm
   # https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png
   # https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png

   History of [[glspl:nn][NN]] can be arguably dated to 1943, when Warren Mcculloch and Walter Pitts devised mathematical model inspired by the biology of central nervous systems of mammals \cite{article--mcculloch-pitts--1943}.

   This inspired the invention of [[gls:perceptron][Perceptron]], created in 1958 by Frank Rosenblatt. Perceptron used very simple model mimicking artificial neuron that was based on mathematical model of Pitts and Mcculloch. Model of [[gls:perceptron][Perceptron]] also encompassed an algorithm direct learning from data.

   # TODO: questionable update!!!
   # also add explanantion about equivalnce to FCNN.

   This simple improvement addressed majority of previously raised concerns and enable the application of [[glspl:nn][NN]] in many different technical domains with moderate success.

   From the beginning [[gls:perceptron][Perceptron]] seemed promising, but it was soon discovered that it had severe limitations. Most prominent voice of criticism was Marvin Minsky, who published book called Perceptrons \cite{book--minsky-papert--1969}. The criticism was centered on the fact that [[gls:perceptron][Perceptron]] model was unable to solve complex problems. Among others the book contained mathematical proof that [[gls:perceptron][Perceptron]] is unable to solve simple XOR problem. More generally the [[gls:perceptron][Perceptron]] is only capable to solve linearly separable problems. Even though according to Minsky this criticism wasn't malicious, it stifled the interest in [[glspl:nn][NN]] for over a decade.
   Interest in [[glspl:nn][NN]] was rejuvenated in the early 80's by the invention of [[gls:bp][BP]] learning algorithm, which enabled the possibility to use multiple [[glspl:perceptron][Perceptrons]] to form networks. [[Glspl:perceptron][Perceptrons]] could also be stacked upon each other to form layers. [[gls:nn][NN]] of this type are commonly called [[gls:mlp][MLP]].

   In 80's and 90's the interest in [[glspl:nn][NN]] plateaued again, and general research of [[gls:ai][AI]] was more focused on other (typically less complex) machine learning techniques. In the realm of [[gls:classification][classification]] problems there were several notable examples ([[gls:svm][SVM]], [[gls:ensemble models][Ensemble models]] and [[gls:random forest][Random Forest]]). [[gls:ai][AI]] research community also developed several other paradigms of [[glspl:nn][NN]] that were similarly inspired by biology of certain aspect of central nervous system but took different approaches. Most important examples were [[gls:som][SOM]] and [[gls:rnn][RNN]] (e.g. [[gls:hopfields networks][Hopfields Networks]]).

   By the year 2000 there was very few research groups that were devoting enough attention to the [[glspl:nn][NN]]. There was also certain disdain for [[glspl:nn][NN]] in academia and [[gls:ai][AI]] research community. Success of [[glspl:nn][NN]] that was promised almost half a century ago was finally encountered around 2006, when the first networks with large number of [[glspl:hidden layer][hidden layer]] was successfully trained. This led to mainstream adaption of umbrella term [[gls:deep learning][DL]] which typically refers to [[gls:dnn][DNN]]. The word [[gls:deep][Deep]] indicates that networks have large number of [[glspl:hidden layer][hidden layer]].

   The key theoretical insight was that to learn complicated functions that can represent high-level abstractions (e.g. vision recognition, language understanding etc.) there is a need for [[gls:deep][deep]] architecture.

   [[glspl:nn][NN]] in the times before [[glspl:dnn][DNN]] typically had only 1 or 2 [[glspl:hidden layer][hidden layer]]. These are today often called shallow networks. Typical Deep Networks can have number of [[glspl:hidden layer][hidden layer]] in order of tens, but in some cases even hundreds [citation]
   # https://www.microsoft.com/en-us/research/publication/foundations-and-trends-in-signal-processing-deep-learning-methods-and-aplications-now-publishers/

   Even though that progress of Neural Network into direction of structures with high number of [[glspl:hidden layer][hidden layer]] was obvious, its training was unsolved technical problem for very long time. There were basically 3 reasons why this breakthrough didn't come sooner.
   1. There were no efficient approach allowing number of [[glspl:hidden layer][hidden layer]] to scale.
   2. There wasn't enough of labeled data to effectively train the [[gls:nn][NN]].
   3. The computational hardware wasn't powerful enough to train sufficiently large and complex networks effectively.

   # TODO: add citation
   First problem was tackled by invention of [[glspl:cnn][CNN]] [citation].
   # LeCunn 1989
   Second problem was solved simply when there was more data available. This was mainly achieved thanks to effort of large companies (Google, Facebook, YouTube, etc.) but also with help of large community of professionals and hobbyists in data sciences.

   Both innovation in computational hardware and improvement of training methods were needed to solve the third problem. One of the technical breakthroughs was utilization of [[gls:gpgpu][GPGPU]]. Thanks to the fact that training process of [[glspl:nn][NN]] is typically large number of simple consequent computations, there is a great potential for parallelization (this is specifically true in case of [[glspl:cnn][CNN]]).



** Structure of Neural Networks
   Description of the Structure of neural network will take already utilized view point that splits analyze into:
    - Model
    - Cost Function
    - Optimization Procedure
*** Model
    Model of [[gls:nn][NN]] is much more complex than the [[gls:linear regression][linear regression]] and for this reason it is useful to divide the analysis into two parts:
    - model of neuron
    - topology of the network
**** Model of neuron
     Typical structure of artificial neuron is shown on Fig. [[fig:neuron_model]]. As it was already mentioned model of neuron was inspired by biology. It computational unit performing operation
     \begin{equation}
     z = g(\boldsymbol{w}^T\boldsymbol{x} + b).
     \end{equation}
     Typical schema is shown on Fig. [[fig:neuron_model]], which depicts inputs, weights bias and activation function.

     #+NAME: fig:neuron_model
     #+CAPTION: Model of artificial neuron used in Perceptron
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__neuron_model.png]]

****** Inputs
       Each neuron has multiple inputs that are combined together to execute some operation. Each input has designated weight assigned to it.

****** Weights
       Inputs of a neuron are weighted by parameters $\boldsymobl{w}$ that are modified during learning process. Each weight gives strength to each individual input into the neuron. The basic idea is that when the weight is small the particular input doesn't influence the output of the neuron very much. It's influence is large in the opposite case.

****** Bias
       Another modifiable parameter is bias that controls influence of the neuron as a whole.

****** TODO Activation Function
       # TODO:
       Function that a neuron gives the [[gls:nn][NN]] the ability to approximate functions.
       In order to be effective learning algorithm it is necessary that the model is able to approximate the arbitrary function. For this reason the [[gls:af][AF]] has to be non-linear.

       # TODO: Update
       In the first iterations of [[gls:mlp][MLP]] evolution it generally had all neurons in hidden and output layer to be similar. Namely activation function was predominantly [[fig:sigmoid][sigmoid]].

       Model of neuron can have multiple different [[glspl:af][AFs]] on its output.


       # TODO: add some better equation describing activation function.
       Activation function  $g()$ of a neuron is performing nonlinear transformation of its linear input

       \begin{equation}
       y=g(z).
       \end{equation}

       #+NAME: fig:relu
       #+CAPTION: Restricted Linear Unit (ReLU)
       #+ATTR_LATEX: :width 4in
       [[./img/figure__2__relu.png]]

       This function has to be nonlinear. Most commonly used [[gls:af][AF]] are [[gls:sigmoid][sigmoid]], softmax and [[gls:relu][relu]].

       # TODO: This doesn't fit here!!!
       Activation function of [[glspl:hidden layer][hidden layer]] is to this day one of the most dynamically evolving components of Neural networks. Currently there is several options but mainly used is Restricted Linear Unit (ReLU) [[fig:relu][relu]] which models following mathematical function

       \begin{equation}
       g(z) = \max \{0,z\}.
       \end{equation}

       #+NAME: fig:sigmoid
       #+CAPTION: Sigmoid activation function
       #+ATTR_LATEX: :width 4in
       [[./img/figure__2__sigmoid.png]]

       # TODO: make seure it fits
       [[Glspl:af][AF]] is still subject of ongoing research, but there are certain recommendation that can be made.
       Different options for [[glspl:af][AFs]] will be described in more detail in following sections.

       [[Gls:sigmoid][Sigmoid]] [[gls:af][AF]] has the advantage that is differentiable over all of its possible values. Another nice property is that its output value is between 1 and 0, which conveniently maps to valid probability distribution.
       Problem with sigmoid is that its gradient becomes really flat on both extremes and as such it slows down the learning process


**** Topology of the Network
     # TODO: feed-forward and recurrent below should be glossaries!!!
     There are several different commonly used topologies. Two most commonly used in [[gls:deep learning][deep learning]] are feed-forward and recurrent. Feed forward networks are characterized by the fact that during execution the information is flowing only in forward direction from inputs to output. The recurrent networks have some kind of feedback loop. Only feed-forward topology will be discussed further.

     Another criterion of topology is how are individual neurons in the network connected. Most commonly are [[glspl:nn][NN]] ordered in layers. In each layer there are multiple neurons and layer are hierarchically stacked. In typical terminology the first layer is called input layer, the last layer is called output layer and the layers in the middle are called hidden.

     Last aspect of topology are interconnections between individual layers and neurons within these layers. Most common scheme is called fully connected where each neuron in hidden layer $l$ has input connections from all neurons from previous layer $l-1$ and its output is connected to input of each neuron in following $l+1$ layer. Entire structure is depicted on Fig. [[fig:net_structure]].

     #+NAME: fig:net_structure
     #+CAPTION: Fully connected feed forward neural network
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__net_structure.png]]

     Neurons in individual layers are dependent on the type of layer. Currently the main difference is in their [[gls:af][af]], which wasn't the case for a long time. For a long time in the history were neurons in [[gls:nn][NN]] equipped with [[gls:sigmoid][sigmoid]] [[glspl:af][AF]]. The main reason being that it is easy to find derivative of [[gls:sigmoid][sigmoid]] function. In the latest years it was found that network composed of neurons with [[gls:sigmoid][sigmoid]] [[glspl:af][AFs]] are difficult to train, mainly because they have tendency to saturate. Solution to this problem are [[gls:relu][ReLU]] [[gls:af][AFs]]. This [[gls:af][AF]] is used in input and hidden layers.

     # TODO: Make sure that each neuron in output layer actually has softmax function!!!
     Neurons in output layer need output that can produce probability distribution that can be used to estimate the probability of individual classes. For this reason most commonly used [[gls:af][AF]] of output neuron is softmax.

     From this point on the term [[gls:nn][NN]] will refer to Feed-forward fully connected Neural Network.


*** TODO Cost function


*** Optimization Procedure

    Optimization procedure or in other words learning of [[gls:nn][NN]] consist of evaluating the cost function for given input data and modifying parameters of the [[gls:nn][NN]] in order to decrees it. Learning algorithm is complex because the [[gls:nn][NN]] has many parameters and it isn't obvious how a change of individual parameter influences the value of the [[gls:cost function][cost function]].

    Technique that is used to solve this problem is called [gls:bp][BP]]. With the knowledge of how the change of parameter affects the value of cost function the optimization procedure that is used to find best possible solution. Is called gradient based optimization. More specifically most commonly is used the [[gls:sgd][SGD]] method. Both [[gls:bp][BP]] algorithm and [[gls:sgd][SGD]] will be described further.


**** Gradient Descent Optimization
     # TODO: This needs hard editing
     Gradient is computed with respect to each input variable. and result of this operation is representing the direction of most steep increase in the output value. Therefore in the heart of every gradient based optimization is an element of applying change proportional to negative gradient of inputs.

     # cost function
     Maximum Likelihood Estimation

     #+NAME: fig:gradient_descent_conture_plot
     #+CAPTION: Depiction of Gradient based optimazation of on the conture plot.
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__gradient_descent_conture_plot.png]]


**** Back-propagation
     Back-propagation is an algorithm that is used to propagate error to individual neurons within the network in order to estimate influence of these neurons on the overall network performance. It is recursive process that is applied through out the network until the input layer is reached. In [[gls:nn][NN]] the [[gls:bp][BP]] is used to computes $\delta_j^l$, where $l$ is layer and $j$ is index of neuron in that layer. Algorithm starts at the output [[gls:nn][NN]], more specifically its cost function.
     \begin{equation}
     \delta^L = \sigma^{'} (z^L) \nabla_x \mathcal{L}
     \end{equation}


**** Regularization

***** Early stoppage

***** Dropout


**** Meta-parameters
***** Learning rate
***** Momentum


*** Shortcomings of Neural Network in image processing
    It was found that general [[gls:fcnn][FCNN]] is not ideal for image processing needs. Even small images typically represents enormous amount of inputs (i.e. image of the size $64 \times 64$ pixels represents 4096 inputs).

    Since each of these inputs has to be connected to all neurons in following layer and weight of each connection has to be memorized, this represents enormous amount of parameters.


    # TODO: this is not substantiated!!!
    # Moreover because during the learning process update of these weights is computed via matrix multiplication for larger images this can be unresolvable problem, which exacerbate with the number of [[glspl:hidden layer][hidden layer]].

    The structure of [[gls:fcnn][FCNN]] has another deficiency for image processing application, which is that it doesn't capture geometric properties of information from input image. In other words because individual layers are fully connected (each output in lower layer is connected to each input in higher layer) networks are not capturing any information about relation of position of individual inputs (image pixels) to each other.

    Third problem is that for higher depth of [[gls:fcnn][FCNN]] increases the likelihood of getting stuck in some local minima.

    All of these problems were solved by the specific type of [[gls:nn][NN]] model called [[glspl:cnn][CNN]] [citation]

    # TODO: Try to find palce for this if it is relevant
    # For example in case of CNNs there is almost no need to process input image before it is used to train the model. Hiearchical extraction of image features that is automatically created by CNN is very advantages in this case.
    # of the fundamental two-dimensional property of image data.



** Convolutional Neural Networks
   [[glspl:cnn][CNN]] are specialized type of [[glspl:nn][NN]] that was originally used in image processing applications. They are arguably most successful models in [[gls:ai][AI]] inspired in biology. Even though they were guided by many different fields, the core design principles were drawn from neuroscience. Since their success in image processing, they were also very successfully deployed in natural language and video processing applications.

   Aforementioned inspiration in biology was based on scientific work of David Hubel and Torsten Wiesel. Hubel and Wisel, who were neurophysiologist, investigated vision system of mammals from late 1950 for several years. In the experiment, that might be considered little gruesome for today's standards, they connected electrodes into brain of anesthetized cat and measured brain response to visual stimuli [Citation]. They discovered that reaction of neurons in visual cortex was triggered by very narrow line of light shined under specific angle on projection screen for cat to see. They determined that individual neurons from visual cortex are reacting only to very specific features of input image. Hubel and Wiesel were awarded the Nobel Prize in Physiology and Medicine in 1981 for their discovery and their finding inspired design of [[glspl:cnn][CNN]].

   # TODO: make sure this is correct!!
   There will be several suppositions made in order to simplify explanation of the concepts involved:
   - It will be presumed that convolutional layer is working with rectangular input data (e.g. images). Even though the Convolutional networks can be also trained to use 1-dimensional input (e.g. sound signal) or 3-dimensional (e.g. [[gls:ct][CT]] scans) etc.
   - The complexity of multiple-channel inputs (i.e. colored images) will be ignored.
   - Each layer requires rectangular input and produces rectangular output per one [[gls:kernel][kernel]].


   # TODO: Add differences oposed to FCNN

*** Structure of CNN
    # TODO: Try to find place for this!!
    # Keeping up with concepts of Neuron and Topology is little more difficult in case of [[gls:cnn][CNN]]. First reason being that the structure of [[gls:cnn][CNN]] is composed of three different types of layers and the second is the fact that some of these layers atypicall and hard to describe by concept of neuron!

    Structure of Convolutional networks is typically composed of three different types of layers. Layer can be of Convolutional, Pooling and [[gls:fc][FC]] type. Each type of layer has different rules for forward and error backward signal propagation.

    There are no precise rules on how the structure of individual layers should be organized. What is typical is that the network has two parts. First part usually called feature extraction that is using combinations of convolutional and pooling layer. Second part called classification is using fully connected layers.

    #+NAME: fig:cnn_structure
    #+CAPTION: Structure of Convolutional Neural Network
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__cnn_structure.png]]

    # TODO: This probably should be deleted
    # Even though there is no strict rule enforcing this, it custom to Network layers can pretty much arbitrarily combine these three types of layers (with exception of Fully-Connected layers, which always have to come last).

**** Convolutional layer

     As the name suggests this layer employs convolution operation. Input into this layer is simply called input. Convolution operation is performed on input with specific filter, which is called [[gls:kernel][kernel]]. Output of convolution operation is typically called [[gls:feature map][feature map]].

     Input into Convolutional layer is either image (in case of first network layer) or [[gls:feature map][feature map]] from previous layer. [[Gls:kernel][kernel]] is typically of square shape and its width can range from 3 to N pixels (typically 3, 5 or 7). [[Gls:feature map][feature map]] is created by convolution of [[gls:kernel][kernel]] over each specified element of input. Convolution is described in more detail in section describing training of [[gls:cnn][CNN]].

     Depending on the size of [[gls:kernel][kernel]] and layer's padding preferences the process of convolution can produce [[gls:feature map][feature map]] of different size than input. When the size of output should be preserved it is necessary to employ [[gls:zero padding][zero padding]] on the edges of input. [[Gls:zero padding][zero padding]] in this case has to add necessary amount of zero elements around the edges of input. This amount is determined by
     \begin{equation}
     p = ((h - 1) / 2)
     \end{equation}

     where h is width of used [[gls:kernel][kernel]]. In opposite case the [[gls:feature map][feature map]] is reduced by the $2*p$. Decreasing of the [[gls:feature map][feature map]] can be in some cases desirable.

     #+NAME: fig:zero_padding
     #+CAPTION: A zero padded 4x4 matrix
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__zero_padding.png]]


     Reduction of [[gls:feature map][feature map]] can go even further in case of use of [[gls:stride][stride]]. Application of [[gls:stride][stride]] specifies by how many input points is traversed when moving to neighboring position in each step. When the [[gls:stride][stride]] is 1, [[gls:kernel][kernel]] is moved by 1 on each step and the resulting size of [[gls:feature map][feature map]] is not affected.

     Each Convolutional layer is typically composition of several different kernels. In other words output of this layer is tensor containing [[gls:feature map][feature map]] for each used [[gls:kernel][kernel]]. Each of these is designed to underline different features of input image. In the first layers these features are typically edges. In following layers the higher the layer the more complex features are captured.

     Each [[gls:kernel][kernel]] that is used is applied to all inputs of the image to produce one [[gls:feature map][feature map]] which basically means that neighboring layers are sharing the same weights. This might not be sufficient in some applications and therefore it is possible to use two other types of connections. [[Gls:locally connected][Locally connected]] which basically means that applied [[gls:kernel][kernel]] is of the same size as the input and [[gls:tiled convolution][tiled convolution]] which means alternation of more than one set of weights on entire input.

     [[Gls:tiled convolution][tiled convolution]] is interesting because with clever combination with [[gls:max-pooling][max-pooling]] explained bellow it allows to train specific feature from multiple angles (in other words invariant to rotation).

     Each convolutional layer has non-linearity on its output that is sometimes also called the [[gls:detector stage][detector stage]].

**** Pooling layer

     This layer typically (more details later) doesn't constitute any learning process but it is used to down-sample size of the input. The Principle is that input is divided into multiple not-overlapping rectangular elements and units within each element are used to create single unit of output. This decreases the size of output layer while preserving the most important information contained in input layer. In other words pooling layer compresses information contained within input.

     Type of operation that is performed on each element determines a type of pooling layer. This operation can be averaging over units within element, selecting maximal value from element or alternatively learned linear combination of units within element. Learned linear combination introduces form of learning into the pooling layer, but it is not very prevalent.

     Selecting of maximal value is most common type of pooling operation and in that case the layer is called [[gls:max-pooling][max-pooling]] accordingly. Positive effect of Max-pooling down-sampling is that extracted features that are learned in convolution are invariant to small shift of input. [[gls:max-pooling][max-pooling]] layer will be used to describe process of training of [[glspl:cnn][CNN]].

     As already mentioned another advantage of Max-pooling arises when combined with [[gls:tiled convolution][tiled convolution]]. To create simple detector that is invariant to rotation it possible to use 4 different kernels that are rotated by 90 degrees among each other and when the [[gls:tiled convolution][tiled convolution]] is used to tile them in groups of 4, the Max-pooling makes sure that resulted [[gls:feature map][feature map]] contains output from the [[gls:kernel][kernel]] with strongest signal (i.e. the one trained for that specific rotation of the feature).

**** Fully-Connected layer

     Fully-Connected layer is formed from classical neurons that can be found in [[gls:fcnn][FCNN]] and it is always located at the end of the layer stack. In other words it is never followed by another Convolutional layer. Depending on the size of whole [[gls:cnn][CNN]] it can have 1 to 3 [[gls:fc][FC]] layers (usually not more than that). Input of the first [[gls:fc][FC]] layer has inputs from all neurons from previous layer to all neurons of following layer (hence fully connected). All [[gls:fc][FC]] layers are together acting as [[gls:fcnn][FCNN]].


*** Training of CNN
    Training process of [[gls:cnn][CNN]] is analogues to [[gls:fcnn][FCNN]] in that both are using [[gls:forward-propagation][forward-propagation]] and [[gls:bp][BP]] phases.

    Situation with [[gls:cnn][CNN]] is more complicated because network is composed of different types of layers and therefore training must accommodate for variability between different layers and also the individual convolution layers are sharing weights across all neurons in each layer.

    # TODO: This needs substantial upgrade !!!
    First phase is the [[gls:forward-propagation][forward-propagation]], where the signal is propagated from inputs of the [[glspl:cnn][CNN]] to its output.
    # TODO: Error function should be probably be called Loss function or maybe Cost function.
    In the last layer the output is compared with desired value by [[gls:loss function][loss function]] and error is estimated.

    Secondly in [[gls:bp][BP]] phase the error is propagated backwards through the network and weights for individual layers are updated by its contribution on the error. Most commonly used algorithm for update of weights is [[gls:gradient descent][gradient descent]]. It is not the only one used but in majority of cases the training algorithm is at least based on [[gls:gradient descent][gradient descent]].

**** Forward Propagation of Convolution Layer
      # TODO: fix this sentence
      Each convolutional layer has inputs. In case that the layer is first, it is network input (i.e individual pixels of image) in other cases, the inputs are outputs from neurons from previous layer (this is typically pooling layer).

      Presuming that input of a layer is of size $N x N$ units and [[gls:kernel][kernel]] is of size $m x m$. Convolution is computed over $(N-m+1) x (N-m+1)$ units (presuming that there is no zero padding).

      Computation of convolution output $x_{ij}^{l}$ is defined as
      \begin{equation}
     x_{ij}^{l}=\sum_{a=0}^{m-1}\sum_{b=0}^{m-1}\omega_{ab}y_{(i+a)(j+b)}^{l-1}
      \end{equation}

 where $i, j \in (0,N-m+1)$, l is index of current layer, $\omega_{ab}$ are weights of layer ([[gls:kernel][kernel]]) and $y_{(i+a)(j+b)}^{l-1}$ is output of previous layer.

      Output of convolutional layer $y_{ij}^{l}$ is computed by squashing of output of convolution operation $x_{ij}^{l}$ through non-linearity:

      \begin{equation}
      y_{ij}^{l}=\sigma(x_{ij}^{l})
      \end{equation}
where $\sigma$ represents this non-linear function.
equation

**** Forward Propagation of Pooling layer (Max-Pooling)

   Feed forward operation of pooling layer is generally very simple and it constitutes in selecting of maximal value within subset
   pooling of multiple inputs into single output.
   Ratio is typically $4 to 1$, which means that input matrix is divided into not-overlapping sub-matrices of size $2 \times 2$ and each of these produces 1 output. Size of sub-matrices can vary and is dependent on size of input, number of layers.

**** Forward Propagation of Fully Connected layer

     Signal is distributed through [[gls:fc][FC]] layer in similar fashion as in Convolutional layer. The main difference is that weights of individual neuron connections are not shared among all neurons in one layer.

**** Backward Propagation of Convolution Layer
     # TOOD: Finish this!!
     # To estimate contribution of convolutional layer to the total error of CNN,
     # there needs to be computed gradient of error function
     Following equasions were lifted from \cite{book--goodfellow--2016}.

     \begin{equation}
     \frac{\partial E} {\partial \omega_{ab}}
     =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial E}{\partial x_{ij}^{l}} \frac{\partial x_{ij}^{l}} {\partial \omega_{ab}}
     =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial E}{\partial x_{ij}^{l}} y_{(i+a)(j+b)}^{l-1}
     \end{equation}

     \begin{equation}
     \frac{\partial E} {\partial x_{ij}^{(l)}}
     =\frac{\partial E} {\partial y_{ij}^{l}} \frac{\partial y_{ij}^{l}} {\partial x_{ij}^{l}}
     =\frac{\partial E} {\partial y_{ij}^{l}} \frac{\partial} {\partial x_{ij}^{l}} \left( \sigma\left(x_{ij}^{l}\right) \right)
     =\frac{\partial E} {\partial y_{ij}^{l}} \sigma' \left( x_{ij}^{l} \right)
     \end{equation}

     \begin{equation}
     \frac{\partial E} {\partial y_{ij}^{l-1}}
     =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial E} {\partial x_{(i-a)(j-b)}^{l}} \frac{\partial x_{(i-a)(j-b)}^{l}} {\partial  y_{ij}^{l-1}}
     =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial E} {\partial x_{(i-a)(j-b)}^{l}} \omega_{ab}
     \end{equation}

**** Backward Propagation of Pooling layer (Max-Pooling)
     As mentioned in section for [[gls:forward-propagation][forward-propagation]], there is no explicit learning process happening in pooling layer. Error is propagated backwards depending on how the signal was propagated forward. In case of [[gls:max-pooling][Max-Pooling]] layer the error is propagated only to the unit with maximal output in [[gls:forward-propagation][forward-propagation]] phase (in other words to the winner of pooling). The error is propagated very sparsely, as result.

     # TODO: Delete the bit about everage pooling it is not necessary!!!
     In case of different pooling method it is adjusted accordingly (i.e. for /average pooling/ the error is propagated according to contribution of individual neurons).

**** Backward Propagation of Fully connected layer
     Training mechanism for [[gls:fc][FC]] layer if following the same principles as in [[gls:fcnn][FCNN]], which is not a subject of detailed discussed here. It is similar to one for convolution layers and from our perspective is only important that the first (last in the sense of [[gls:bp][BP]]) [[gls:fc][FC]] layer propagates error gradient of each neuron in it, that is then send to all neurons in preceding (following in the direction of [[gls:bp][BP]]) layer.


*** Advantages of CNN
    # TODO: Find out what I meant by this!!
    # Number of parameters
    # computational demand
    To further highlight the difference between [[gls:fcnn][FCNN]] and [[gls:cnn][CNN]] it is worth to compare the case of 2 neighboring layers.
    Lets have gray scale input image of size 32x32 pixels and following layer will be convolutional with 6 feature maps of size 28x28. Kernels used in this convolutional layer will have the size of 5x5. In this case we have totally $(5 * 5 + 1) * 6 = 156$ parameters between the two layers.
    If we would like to create equivalent connection between two layers of [[glspl:fcnn][FCNN]], then it would have mean $(32 * 32 + 1) * 28 * 28 = 803600$ connections (parameters). Which means that difference between the two is of ~5000 ratio.
    This difference would rise exponentially with larger images or with more color channels. When input size of the image changes to 64x64 and it has [[gls:rgb][RGB]] color then [[glspl:fcnn][FCNN]] would requires $(64 * 64 * 3 + 1) * 28 * 28 = 9634576$ connections (parameters). In the same case the [[gls:cnn][CNN]] only needs $(5 * 5 * 3 + 1) * 6 = 456$ parameters. Which is difference of ~20000 factor.
    Just to elaborate, in case that [[gls:cnn][CNN]] would be used to process video. Analogically to previous examples in case of moving image in time the number of parameters raises linearly with number of images in analyzed video.

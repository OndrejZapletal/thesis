* Neural Networks
  The term [[Gls:nn][NN]] is very general and it describes broad family of models. Its history goes back over 75 years.
  # TODO: This seem to me that it is to condecsending!!!
  # This situation often leads to confusion of some concepts.
  [[Gls:nn][NN]] is a distributed and parallel model that is capable to approximate complex non-linear functions. Network is composed from multiple computational units called neurons that are assembled in particular topology.

** History
   # TODO: assign citation
   # (citation) http://web.csulb.edu/~cwallis/artificialn/History.htm
   # https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png
   # https://commons.wikimedia.org/wiki/File:ArtificialNeuronModel_english.png

   History of [[glspl:nn][NN]] can be arguably dated from 1943, when Warren Mcculloch and Walter Pitts devised mathematical model inspired by the biology of central nervous systems of mammals \cite{article--mcculloch-pitts--1943}.

   This inspired the invention of [[gls:perceptron][Perceptron]], created in 1958 by Frank Rosenblatt. Perceptron used very simple model mimicking biological neuron that was based on mathematical model of Pitts and Mcculloch. Definition of the [[gls:perceptron][Perceptron]] model also described an algorithm for direct learning from data.

   # This simple improvement addressed majority of previously raised concerns and enable the application of [[glspl:nn][NN]] in many different technical domains with moderate success.

   In the beginning [[gls:perceptron][Perceptron]] seemed very promising, but it was soon discovered that it had severe limitations. Most prominent voice of criticism was Marvin Minsky, who published book called Perceptrons \cite{book--minsky-papert--1969}. The criticism was centered on the fact that [[gls:perceptron][Perceptron]] model was unable to solve complex problems. Among others the book contained mathematical proof that [[gls:perceptron][Perceptron]] is unable to solve simple XOR problem. More generally the [[gls:perceptron][Perceptron]] is only capable to solve linearly separable problems. Even though according to Minsky this criticism wasn't malicious, it in effect stifled the interest in [[glspl:nn][NN]] for over a decade.

   Interest in [[glspl:nn][NN]] was rejuvenated in the early 80's, when it was shown that any previously raised deficiencies can solved by usage of multiple units. This was later exacerbated by invention of [[gls:bp][BP]] learning algorithm, which enabled the possibility to gather neurons into groups called layers, which can be stacked into hierarchical structures to form a network. [[Gls:nn][NN]] of this type are commonly called [[gls:mlp][MLP]].

   In 80's and 90's the interest in [[glspl:nn][NN]] plateaued again, and general research of [[gls:ai][AI]] was more focused on other (typically less complex) machine learning techniques. In the realm of [[gls:classification][classification]] problems there were several notable examples ([[gls:svm][SVM]], [[gls:ensemble model][Ensemble model]] and [[gls:random forest][Random Forest]]). [[gls:ai][AI]] research community also developed several other paradigms of [[glspl:nn][NN]] that were similarly inspired by biology of certain aspect of central nervous system but took different approaches. Most important examples were [[gls:som][SOM]] and [[gls:rnn][RNN]] (e.g. [[glspl:hopfield network][Hopfield Networks]]).

   By the year 2000 there was very few research groups that were devoting enough attention to the [[glspl:nn][NN]]. There was also certain disdain for [[glspl:nn][NN]] in academia and [[gls:ai][AI]] research community. Success of [[glspl:nn][NN]] that was promised almost half a century ago was finally encountered around 2006, when the first networks with large number of [[glspl:hidden layer][hidden layer]] was successfully trained. This led to mainstream adaption of umbrella term [[gls:deep learning][DL]] which typically refers to [[gls:dnn][DNN]]. The word [[gls:deep][Deep]] indicates that networks have large number of [[glspl:hidden layer][hidden layer]].

   The key theoretical insight was that to learn complicated functions that can represent high-level abstractions (e.g. vision recognition, language understanding etc.) there is a need for [[gls:deep][deep]] architecture.

   [[glspl:nn][NN]] in the times before [[glspl:dnn][DNN]] typically had only 1 or 2 [[glspl:hidden layer][hidden layer]]. These are today often called shallow networks. Typical Deep Networks can have number of [[glspl:hidden layer][hidden layer]] in order of tens, but in some cases even hundreds \cite{article__He_Zhang__2015}
   # https://www.microsoft.com/en-us/research/publication/foundations-and-trends-in-signal-processing-deep-learning-methods-and-aplications-now-publishers/

   Even though that progress of Neural Network into direction of structures with high number of [[glspl:hidden layer][hidden layer]] was obvious, its training was unsolved technical problem for very long time. There were basically 3 reasons why this breakthrough didn't come sooner.
   1. There were no technique allowing the number of [[glspl:hidden layer][hidden layer]] to scale.
   2. There wasn't enough of labeled data necessary to train the [[gls:nn][NN]].
   3. The computational hardware wasn't powerful enough to train sufficiently large and complex networks effectively.

   First problem was tackled by invention of [[glspl:cnn][CNN]] \cite{article__lecun__1989}.
   Second problem was solved simply when there was more data available. This was mainly achieved thanks to effort of large companies (Google, Facebook, YouTube, etc.) but also with help of large community of professionals and hobbyists in data sciences.

   # TODO: maybe make sure that you still stay behind this!
   Both innovation in computational hardware and improvement of training methods were needed to solve the third problem. One of the technical breakthroughs was utilization of [[glspl:gpu][GPU]] for the demanding computation involved in training of a complex network. Thanks to the fact that training process of [[glspl:nn][NN]] is typically large number of simple consequent computations, there is a great potential for parallelization.

** Structure of Neural Networks
   Description of [[gls:nn][NN]] structure will follow the convention laid out in the description of learning algorithm. Meaning that a description of the learning algorithm is composed of model, cost function and optimization procedure. The difference comes into play with the fact that model of [[gls:nn][NN]] is much more complex than the model [[gls:linear regression][linear regression]]. Therefore, the analysis is divided into model of neuron and topology of the network.

*** Model of Neuron
    Typical structure of artificial neuron is shown on Figure [[fig:neuron_model]]. As it was already mentioned model of neuron was inspired by biology. It computational unit performing operation
    \begin{equation}
    y = g(\boldsymbol{w}^T\boldsymbol{x} + b).
    \end{equation}
    # TODO: two references to the same thing right bellow each other!
    Typical schema is shown on Figure [[fig:neuron_model]], which depicts inputs, weights bias and activation function.

    #+NAME: fig:neuron_model
    #+CAPTION: Model of artificial neuron
    [[./img/figure__2__neuron_model.png]]

**** Inputs
     Each neuron has multiple inputs that are combined together to execute some operation. Each input has designated weight assigned to it.

**** Weights
     Inputs of a neuron are weighted by parameters $\boldsymbol{w}$ that are modified during learning process. Each weight gives strength to each individual input into the neuron. The basic idea is that when the weight is small the particular input doesn't influence the output of the neuron very much. Its influence is large in the opposite case.

**** Bias
     Another modifiable parameter is bias $b$ that controls influence of the neuron as a whole.

**** Activation Function
     For [[gls:nn][NN]] to approximate nonlinear function each neuron has to perform nonlinear transformation of its input. This is done with activation function that preforms nonlinear transformation
     \begin{equation}
     y=g(z).
     \end{equation}
     There are several different commonly used activation functions. Its usage depends on the type of network and also on the type of layer in which they operate.

     One of the oldest and historically most commonly used [[gls:af][AF]] if [[gls:sigmoid][sigmoid]] function. It is defined by
     \begin{equation}
     g(z)=\frac {1} {1+e^{-z}}.
     \end{equation}
     Problem with [[gls:sigmoid][sigmoid]] is that its gradient becomes really flat on both extremes and as such it slows down the learning process \cite{article__krizhevsky__2012}.

     Another activation function is [[gls:hyperbolic tangent][hyperbolic tangent]]. It is less common but still used in specific types of networks. It is defined as
     \begin{equation}
     g(z)=tanh(-z).
     \end{equation}
     Hyperbolic tangent function is not used in feed forward [[gls:nn][NN]] as often, but it is largely used in [[gls:rnn][RNN]].

     Currently most frequently used activation function is [[gls:relu][ReLU]]. It is very commonly used in both convolutional and fully connected layers. It is defined by
     \begin{equation}
     g(z)=\max \{0,z\}.
     \end{equation}
     It has a drawback because it is not differentiable for $z=0$, but it is not a problem in software implementation and one of its biggest advantages is that it can learn very quickly.

     All three activation functions are illustrated in Figure \ref{fig:activation}.

     #+INCLUDE: activation_function.org

*** Topology of the Network
    # TODO: feed-forward and recurrent below should be glossaries!!!
    There are several different commonly used topologies. Two most commonly used in [[gls:deep learning][deep learning]] are feed-forward and recurrent. Feed forward networks are characterized by the fact that during activation the information flows only in forward direction from inputs to output. A recurrent network has some sort of feedback loop.

    Another criterion of topology is how are individual neurons in the network connected. Most commonly are [[glspl:nn][NN]] ordered in layers. In each layer there can be from /1/ to /n/ neurons. Layers are hierarchically stacked. In typical terminology the first layer is called input layer, the last layer is called output layer and the layers in-between are called hidden.

    Description of the network rests on interconnections between individual layers. Most common scheme is called fully connected where each neuron in hidden layer $l$ has input connections from all neurons from previous layer $l-1$ and its output is connected to input of each neuron in following $l+1$ layer. Entire structure is illustrated on Figure [[fig:net_structure]].

    From this point on the term [[gls:nn][NN]] will refer to Feed-forward fully connected Neural Network.

    #+NAME: fig:net_structure
    #+CAPTION: Fully connected Feed Forward Neural Network
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__net_structure.png]]

    Types of neurons are dependent on the type of the layer. Currently the main difference is in their [[gls:af][af]], which wasn't the case for a long time. Historically all layers had neurons with [[gls:sigmoid][sigmoid]] [[gls:af][AF]]. It was mainly because the output [[gls:sigmoid][sigmoid]] layer can be easily mapped onto probability distribution, since it acquires vales between 0 and 1. Only relatively recently[fn:1] it was found that network composed of neurons with [[gls:relu][ReLU]] [[gls:af][AFs]] in the hidden layers can be trained very quickly and are more resistant against over-fitting. [[Glspl:af][AF]] are still subject of ongoing research.

    Neurons in output layer need output that can produce probability distribution that can be used to estimate the probability of individual classes. For this reason, most commonly used [[gls:af][AF]] of output neuron is called [[gls:softmax][softmax]]. [[Gls:softmax][Softmax]] is normalized exponential function. It is used to represent probability of an element being member of class $j$ as
    \begin{equation} \label{eq:softmax}
    g(z)_j = \frac {e^{z_j}} {\sum_{K}^{k=1} {e^{z_k}}},
    \end{equation}
    where $K$ is total number of classes.


[fn:1] In the last decade which is relatively recently in the grand scheme of [[gls:nn][NN]] history.

*** TODO Cost Function
    [[Glspl:cost function][Cost functions]] in [[glspl:nn][NN]] are vast subject that exceeds scope of this thesis. One of the most common cost function used in [[glspl:nn][NN]] for [[gls:classification][classification]] into multiple classes is [[gls:categorical cross entropy][categorical cross entropy]]. For [[gls:softmax][softmax]] [[gls:af][AF]] defined in equation \ref{eq:softmax} it is defined as
    \begin{equation}
    \mathcal{L}(\boldsymbol{z},\boldsymbol{y}) = - \frac {1}{n} \sum_{i=1}^{n} y^{(i)}\ln g(z^{(i)}) + (1 - y^{(i)}) \ln (1 - g(z^{(i)})),
    \end{equation}
    where $y^{(i)}$ if correct class of the instance and $n$ is total number of instances.

*** TODO Optimization Procedure

    # Optimization procedure or in other words learning of [[gls:nn][NN]] consist of evaluating the cost function for given input data and modifying parameters of the [[gls:nn][NN]] in order to decrees it.


    Every optimization procedure for [[gls:nn][NN]] is based on gradient descent. In other words, it is iterative process that is trying to lower training error of the network by differentiating of [[gls:cost function][cost function]] and adjusting parameters of the model by following the negative gradient.

    The problem is that [[gls:cost function][cost function]] of entire network is very complex and has many parameters. To find the gradient of the cost function it is necessary to go through all of the units in the network. Technique that is used to solve this problem is called [[gls:bp][BP]]. [[Gls:bp][BP]] if often confused to be complete learning algorithm which is not the case, it is only the method to compute the gradient \cite{book--goodfellow--2016}.

    # TODO: maybe word this better
    Algorithm that facilitates the learning is the most important piece of the puzzle. There are many different variations on the gradient descent method. Following definitions are taken from \cite{article__dozat__2015}.
    # With the knowledge of how the change of parameter affects the value of cost function the optimization procedure that is used to find best possible solution.

    # Is called gradient based optimization. More specifically most commonly is used the [[gls:sgd][SGD]] method.

    # Both [[gls:bp][BP]] algorithm and [[gls:sgd][SGD]] will be described further.


**** Gradient Descent
     It is one of the simplest yet very robust learning algorithm.
     \begin{multline} \label{eq:sgd}
     g_t \leftarrow \nabla_{\theta_{t-1}} f(\theta_{t-1}) \\
     \theta_{t-1} \leftarrow \theta_{t-1} - \eta g_t
     \end{multline}

     Parameter $\eta$ is often called learning rate. It determines how quickly are updated parameters of the network. Simple gradient descent has the shortcoming that update of parameters is always exactly proportional to change of gradient. This might become a problem when the gradient change slows down.

**** Adam
     Adam is much more complex learning algorithm. It combines $L_2$ norm and momentum based optimization. Theoretically it should have much better performance than classical Gradient Descent but it can be unstable.
     \begin{multline} \label{eq:adam}
     g_t \leftarrow \nabla_{\theta_{t-1}} f(\theta_{t-1}) \\
     m_t \leftarrow \mu m_{t-1} + (1- \mu) g_t \\
     \hat{m_t} \leftarrow \frac {m_t} {1 - \mu^t} \\
     n_t \leftarrow \nu n_{t-1} + (1 - \nu) g_{t}^{2} \\
     \hat{n_t} \leftarrow \frac {n_t} {1 - \nu^t} \\
     \theta_{t-1} \leftarrow \theta_{t-1} - \eta \frac {\hat{m_t}} {\sqrt{n_t} + \varepsilon}
     \end{multline}

**** Nadam
     Nadam is further improvement of Adam that extends it with Nesterov acceleration trick that should improve speed of convergence.
\begin{multline} \label{eq:nadam}
g_t \leftarrow \nabla_{\theta_{t-1}} f(\theta_{t-1}) \\
\hat{g_t} \leftarrow \frac {g_t} {1 - \prod_{i=1}^{t} \mu_i} \\
m_t \leftarrow \mu m_{t-1} + (1 - \mu) g_t \\
\hat{m_t} \leftarrow \frac {m_t} {1 - \prod_{i=1}^{t} \mu_i} \\
n_t \leftarrow \nu n_{t-1} + (1 - \nu) g_{t}^{2} \\
\hat{n_t} \leftarrow \frac {n_t} {1 - \nu^t} \\
\bar{m_t} \leftarrow (1 - \mu_t)\hat{g_t} + \mu_{t+1} \hat{m_t} \\
\theta_{t-1} \leftarrow \theta_{t-1} - \eta \frac {\bar{m_t}} {\sqrt{n_t} + \varepsilon}
\end{multline}

**** TODO Gradient Descent Optimization                            :noexport:
     # TODO: This needs hard editing
     Gradient is computed with respect to each input variable. and result of this operation is representing the direction of most steep increase in the output value. Therefore in the heart of every gradient based optimization is an element of applying change proportional to negative gradient of inputs.

     # cost function
     Maximum Likelihood Estimation

     #+NAME: fig:gradient_descent_conture_plot
     #+CAPTION: Depiction of Gradient based optimazation of on the conture plot.
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__gradient_descent_conture_plot.png]]

**** TODO Back-propagation                                         :noexport:
     Back-propagation is an algorithm that is used to propagate error to individual neurons within the network in order to estimate influence of these neurons on the overall network performance. It is recursive process that is applied through out the network until the input layer is reached. In [[gls:nn][NN]] the [[gls:bp][BP]] is used to computes $\delta_j^l$, where $l$ is layer and $j$ is index of neuron in that layer. Algorithm starts at the output [[gls:nn][NN]], more specifically its cost function.
     \begin{equation}
     \delta^L = \sigma^{'} (z^L) \nabla_x \mathcal{L}
     \end{equation}

**** TODO Meta-parameters                                          :noexport:
***** Learning rate
***** Momentum


*** TODO Shortcomings of Neural Network in Image Processing
    It was found that general [[gls:fcnn][FCNN]] is not ideal for image processing needs. Even small images typically represents enormous amount of inputs (i.e. image of the size $64 \times 64$ pixels represents 4096 inputs).

    Since each of these inputs has to be connected to all neurons in following layer and weight of each connection has to be memorized, this represents enormous amount of parameters.


    # TODO: this is not substantiated!!!
    # Moreover because during the learning process update of these weights is computed via matrix multiplication for larger images this can be unresolvable problem, which exacerbate with the number of [[glspl:hidden layer][hidden layer]].

    The structure of [[gls:fcnn][FCNN]] has another deficiency for image processing application, which is that it doesn't capture geometric properties of information from input image. In other words, because individual layers are fully connected (each output in lower layer is connected to each input in higher layer) networks are not capturing any information about relation of position of individual inputs (image pixels) to each other.

    Third problem is that for higher depth of [[gls:fcnn][FCNN]] increases the likelihood of getting stuck in some local minima.

    All of these problems were solved by the specific type of [[gls:nn][NN]] model called [[glspl:cnn][CNN]] \cite{article__lecun__1989}.

    # TODO: Try to find palce for this if it is relevant
    # For example in case of CNNs there is almost no need to process input image before it is used to train the model. Hiearchical extraction of image features that is automatically created by CNN is very advantages in this case.
    # of the fundamental two-dimensional property of image data.



# *** Fully Connected Neural Network
#     This learning algorithm is often called by several different names. These names usually indicate the perspective from which it is described. Historically the most common name was [[gls:mlp][MLP]]. In the recent years the name [[gls:fcnn][FCNN]] is used more often to differentiate it from other popular [[gls:nn][NN]] algorithms as for example [[gls:rnn][RNN]]. In the same context it is also sometimes called [[gls:ffnn][FFNN]].

    # TODO: This needs an ending!!!
    # Different fields of scientific inquiry adopted these principles in different ways and as a consequence the term [[glspl:nn][NN]] is not very descriptive. There following description will be used.

    # In some literature there might be small difference between [[gls:mlp][MLP]] and [[gls:fcnn][FCNN]] but in this document they will be used interchangeably.


    # Word this better!!
    # In its infancy the goal was mostly aimed to capture mechanisms present in the pinnacle of intelligent organisms, which is human brain.

    # This view is useful in describing motivations, but it is necessary to point out that equivalence between [[gls:nn][NN]] models and biology grew less important as the researched progressed. At the same time there is one notable exception to this which are [[glspl:cnn][CNN]] that were heavily influenced by study of visual systems of mammals.



       # TODO: This might be interesting to point out, but it is not necessary!!!
       # Neural networks are definitely inspired by biology. First attempts to created model of neuron had multiple elements equivalent with neurons of human brain. As time progressed this equivalence sized to being as important and modern neural networks models correspond to h their biological counterparts only superficially.

       # *** Nonlinear models

       # Problem with nonlinear models is that their optimization doesn't result in quadratic optimization therefore it is not possible to guarantee the finding of optimal solution. It is necessary to use iterative approach to solving the optimization problem

    # ################################################################################################
    # Structure of Neural Networks
    # ################################################################################################

# **** Model

# ****** Perceptron
#        Computational model that is using single unit of neuron. It defines operation for prediction and also learning algorithm that it enables to compute. Later adopted to [[gls:mlp][MLP]] which uses it as building block.

       # TODO:
       # This was already described in introduction chapter and therefore it might be good idea to cosolidate the two.

       # Again because it can widely vary the discussion is only concentrated on model of neuron in [[gls:mlp][MLP]].


       # TODO: Try to compose this into the structure
       # There are 3 things that define Neural network

    # As the names suggest this model is formed by combination of multiple units typically similar to [[gls:perceptron][Perceptron]] that are group into layers which are fully interconnected in between (i.e. output of each neuron in one layer is connected to all inputs of neurons in following layer). Not all neurons in the network are the same. There are typically two types of neurons. Neurons in the output layer (called [[glspl:output unit][output units]]) and neurons in the rest of the network (called [[glspl:hidden unit][hidden unit]]). Neurons in input layer are of the same type as neurons in hidden layer even thought they are not hidden. Only difference between the two types of neurons is in their activation function.

    # [[Gls:activation function][Activation function]] is a component of neural network that is subject of most dynamic research. New [[glspl:activation function][activation function]] are developed on regular basis.


    # TODO: This doesn't really fit here!!!
    # Simple three layered fully connected neural network, given sufficient number of neurons in each layer, is able to approximate arbitrary continues function with arbitrary accuracy \cite{article--Kurkova--1992}.
** Convolutional Neural Networks
   [[glspl:cnn][CNN]] are specialized type of [[glspl:nn][NN]] that was originally used in image processing applications. They are arguably most successful models in [[gls:ai][AI]] inspired in biology. Even though they were guided by many different fields, the core design principles were drawn from neuroscience. Since their success in image processing, they were also very successfully deployed in natural language and video processing applications.

   Aforementioned inspiration in biology was based on scientific work of David Hubel and Torsten Wiesel. Hubel and Wisel, who were neurophysiologist, investigated vision system of mammals from late 1950 for several years. In the experiment, that might be considered little gruesome for today's standards, they connected electrodes into brain of anesthetized cat and measured brain response to visual stimuli \cite{article__hubel__1959}. They discovered that reaction of neurons in visual cortex was triggered by very narrow line of light shined under specific angle on projection screen for cat to see. They determined that individual neurons from visual cortex are reacting only to very specific features of input image. Hubel and Wiesel were awarded the Nobel Prize in Physiology and Medicine in 1981 for their discovery and their finding inspired design of [[glspl:cnn][CNN]].

   # TODO: make sure this is correct!!
   In the following text is presumed that convolutional layer is working with rectangular input data (e.g. images). Even though the Convolutional networks can be also trained to use 1-dimensional input (e.g. sound signal) or 3-dimensional (e.g. [[gls:ct][CT]] scans) etc.

   # TODO: Add differences oposed to FCNN

*** Structure of CNN
    # TODO: Try to find place for this!!
    # Keeping up with concepts of Neuron and Topology is little more difficult in case of [[gls:cnn][CNN]]. First reason being that the structure of [[gls:cnn][CNN]] is composed of three different types of layers and the second is the fact that some of these layers atypicall and hard to describe by concept of neuron!

    Structure of Convolutional networks is typically composed of three different types of layers. Layer can be of Convolutional, Pooling and [[gls:fc][FC]] type. Each type of layer has different rules for forward and error backward signal propagation.

    There are no precise rules on how the structure of individual layers should be organized. What is typical is that the network has two parts. First part usually called feature extraction that is using combinations of convolutional and pooling layer. Second part called classification is using fully connected layers.

    #+NAME: fig:cnn_structure
    #+CAPTION: Structure of Convolutional Neural Network
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__cnn_structure.png]]

    # TODO: This probably should be deleted
    # Even though there is no strict rule enforcing this, it custom to Network layers can pretty much arbitrarily combine these three types of layers (with exception of Fully-Connected layers, which always have to come last).

**** Convolutional layer

     As the name suggests this layer employs convolution operation. Input into this layer is simply called input. Convolution operation is performed on input with specific filter, which is called [[gls:kernel][kernel]]. Output of convolution operation is typically called [[gls:feature map][feature map]].

     Input into Convolutional layer is either image (in case of first network layer) or [[gls:feature map][feature map]] from previous layer. [[Gls:kernel][kernel]] is typically of square shape and its width can range from 3 to N pixels (typically 3, 5 or 7). [[Gls:feature map][feature map]] is created by convolution of [[gls:kernel][kernel]] over each specified element of input. Convolution is described in more detail in section describing training of [[gls:cnn][CNN]].

     Depending on the size of [[gls:kernel][kernel]] and layer's padding preferences the process of convolution can produce [[gls:feature map][feature map]] of different size than input. When the size of output should be preserved it is necessary to employ [[gls:zero padding][zero padding]] on the edges of input. [[Gls:zero padding][zero padding]] in this case has to add necessary amount of zero elements around the edges of input. This amount is determined by
     \begin{equation}
     p = ((h - 1) / 2)
     \end{equation}

     where h is width of used [[gls:kernel][kernel]]. In opposite case the [[gls:feature map][feature map]] is reduced by the $2*p$. Decreasing of the [[gls:feature map][feature map]] can be in some cases desirable.

     #+NAME: fig:zero_padding
     #+CAPTION: A zero padded 4x4 matrix
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__zero_padding.png]]


     Reduction of [[gls:feature map][feature map]] can go even further in case of use of [[gls:stride][stride]]. Application of [[gls:stride][stride]] specifies by how many input points is traversed when moving to neighboring position in each step. When the [[gls:stride][stride]] is 1, [[gls:kernel][kernel]] is moved by 1 on each step and the resulting size of [[gls:feature map][feature map]] is not affected.

     Each Convolutional layer is typically composition of several different kernels. In other words, output of this layer is tensor containing [[gls:feature map][feature map]] for each used [[gls:kernel][kernel]]. Each of these is designed to underline different features of input image. In the first layers these features are typically edges. In following layers the higher the layer, the more complex features are captured.

     Each [[gls:kernel][kernel]] that is used is applied to all inputs of the image to produce one [[gls:feature map][feature map]] which basically means that neighboring layers are sharing the same weights. This might not be sufficient in some applications and therefore it is possible to use two other types of connections. [[Gls:locally connected][Locally connected]] which basically means that applied [[gls:kernel][kernel]] is of the same size as the input and [[gls:tiled convolution][tiled convolution]] which means alternation of more than one set of weights on entire input.

     [[Gls:tiled convolution][tiled convolution]] is interesting because with clever combination with [[gls:max-pooling][max-pooling]] explained bellow it allows to train specific feature from multiple angles (in other words invariant to rotation).

     Each convolutional layer has non-linearity on its output that is sometimes also called the [[gls:detector stage][detector stage]].

**** Pooling layer

     This layer typically (more details later) doesn't constitute any learning process but it is used to down-sample size of the input. The Principle is that input is divided into multiple not-overlapping rectangular elements and units within each element are used to create single unit of output. This decreases the size of output layer while preserving the most important information contained in input layer. In other words, pooling layer compresses information contained within input.

     Type of operation that is performed on each element determines a type of pooling layer. This operation can be averaging over units within element, selecting maximal value from element or alternatively learned linear combination of units within element. Learned linear combination introduces form of learning into the pooling layer, but it is not very prevalent.

     Selecting of maximal value is most common type of pooling operation and in that case the layer is called [[gls:max-pooling][max-pooling]] accordingly. Positive effect of Max-pooling down-sampling is that extracted features that are learned in convolution are invariant to small shift of input. [[gls:max-pooling][max-pooling]] layer will be used to describe process of training of [[glspl:cnn][CNN]].

     As already mentioned another advantage of Max-pooling arises when combined with [[gls:tiled convolution][tiled convolution]]. To create simple detector that is invariant to rotation it possible to use 4 different kernels that are rotated by 90 degrees among each other and when the [[gls:tiled convolution][tiled convolution]] is used to tile them in groups of 4, the Max-pooling makes sure that resulted [[gls:feature map][feature map]] contains output from the [[gls:kernel][kernel]] with strongest signal (i.e. the one trained for that specific rotation of the feature).

**** Fully-Connected layer

     Fully-Connected layer is formed from classical neurons that can be found in [[gls:fcnn][FCNN]] and it is always located at the end of the layer stack. In other words, it is never followed by another Convolutional layer. Depending on the size of whole [[gls:cnn][CNN]] it can have 1 to 3 [[gls:fc][FC]] layers (usually not more than that). Input of the first [[gls:fc][FC]] layer has inputs from all neurons from previous layer to all neurons of following layer (hence fully connected). All [[gls:fc][FC]] layers are together acting as [[gls:fcnn][FCNN]].


*** TODO Training of CNN
    Training process of [[gls:cnn][CNN]] is analogues to [[gls:fcnn][FCNN]] in that both are using [[gls:forward-propagation][forward-propagation]] and [[gls:bp][BP]] phases.

    Situation with [[gls:cnn][CNN]] is more complicated because network is composed of different types of layers and therefore training must accommodate for variability between different layers and also the individual convolution layers are sharing weights across all neurons in each layer.

    # TODO: This needs substantial upgrade !!!
    First phase is the [[gls:forward-propagation][forward-propagation]], where the signal is propagated from inputs of the [[glspl:cnn][CNN]] to its output.
    # TODO: Error function should be probably be called Loss function or maybe Cost function.
    In the last layer the output is compared with desired value by [[gls:loss function][loss function]] and error is estimated.

    Secondly in [[gls:bp][BP]] phase the error is propagated backwards through the network and weights for individual layers are updated by its contribution on the error. Most commonly used algorithm for update of weights is [[gls:gradient descent][gradient descent]]. It is not the only one used but in majority of cases the training algorithm is at least based on [[gls:gradient descent][gradient descent]].

**** Forward Propagation of Convolution Layer
      # TODO: fix this sentence
      Each convolutional layer has inputs. In case that the layer is first, it is network input (i.e individual pixels of image) in other cases, the inputs are outputs from neurons from previous layer (this is typically pooling layer).

      Presuming that input of a layer is of size $N x N$ units and [[gls:kernel][kernel]] is of size $m x m$. Convolution is computed over $(N-m+1) x (N-m+1)$ units (presuming that there is no zero padding).

      Computation of convolution output $x_{ij}^{l}$ is defined as
      \begin{equation}
     x_{ij}^{l}=\sum_{a=0}^{m-1}\sum_{b=0}^{m-1}\omega_{ab}y_{(i+a)(j+b)}^{l-1}
      \end{equation}

 where $i, j \in (0,N-m+1)$, l is index of current layer, $\omega_{ab}$ are weights of layer ([[gls:kernel][kernel]]) and $y_{(i+a)(j+b)}^{l-1}$ is output of previous layer.

      Output of convolutional layer $y_{ij}^{l}$ is computed by squashing of output of convolution operation $x_{ij}^{l}$ through non-linearity:

      \begin{equation}
      y_{ij}^{l}=\sigma(x_{ij}^{l})
      \end{equation}
      where $\sigma$ represents this non-linear function.
equation

**** Forward Propagation of Pooling layer (Max-Pooling)

   Feed forward operation of pooling layer is generally very simple and it constitutes in selecting of maximal value within subset
   pooling of multiple inputs into single output.
   Ratio is typically $4 to 1$, which means that input matrix is divided into not-overlapping sub-matrices of size $2 \times 2$ and each of these produces 1 output. Size of sub-matrices can vary and is dependent on size of input, number of layers.

**** Forward Propagation of Fully Connected layer

     Signal is distributed through [[gls:fc][FC]] layer in similar fashion as in Convolutional layer. The main difference is that weights of individual neuron connections are not shared among all neurons in one layer.

**** Backward Propagation of Convolution Layer
     # TOOD: Finish this!!
     # To estimate contribution of convolutional layer to the total error of CNN,
     # there needs to be computed gradient of error function
     Following equasions were lifted from \cite{book--goodfellow--2016}.

     \begin{equation}
     \frac{\partial E} {\partial \omega_{ab}}
     =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial E}{\partial x_{ij}^{l}} \frac{\partial x_{ij}^{l}} {\partial \omega_{ab}}
     =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial E}{\partial x_{ij}^{l}} y_{(i+a)(j+b)}^{l-1}
     \end{equation}

     \begin{equation}
     \frac{\partial E} {\partial x_{ij}^{(l)}}
     =\frac{\partial E} {\partial y_{ij}^{l}} \frac{\partial y_{ij}^{l}} {\partial x_{ij}^{l}}
     =\frac{\partial E} {\partial y_{ij}^{l}} \frac{\partial} {\partial x_{ij}^{l}} \left( \sigma\left(x_{ij}^{l}\right) \right)
     =\frac{\partial E} {\partial y_{ij}^{l}} \sigma' \left( x_{ij}^{l} \right)
     \end{equation}

     \begin{equation}
     \frac{\partial E} {\partial y_{ij}^{l-1}}
     =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial E} {\partial x_{(i-a)(j-b)}^{l}} \frac{\partial x_{(i-a)(j-b)}^{l}} {\partial  y_{ij}^{l-1}}
     =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial E} {\partial x_{(i-a)(j-b)}^{l}} \omega_{ab}
     \end{equation}

**** Backward Propagation of Pooling layer (Max-Pooling)
     As mentioned in section for [[gls:forward-propagation][forward-propagation]], there is no explicit learning process happening in pooling layer. Error is propagated backwards depending on how the signal was propagated forward. In case of [[gls:max-pooling][Max-Pooling]] layer the error is propagated only to the unit with maximal output in [[gls:forward-propagation][forward-propagation]] phase (in other words to the winner of pooling). The error is propagated very sparsely, as result.

     # TODO: Delete the bit about everage pooling it is not necessary!!!
     In case of different pooling method, it is adjusted accordingly (i.e. for /average pooling/ the error is propagated according to contribution of individual neurons).

**** Backward Propagation of Fully connected layer
     Training mechanism for [[gls:fc][FC]] layer if following the same principles as in [[gls:fcnn][FCNN]], which is not a subject of detailed discussed here. It is similar to one for convolution layers and from our perspective is only important that the first (last in the sense of [[gls:bp][BP]]) [[gls:fc][FC]] layer propagates error gradient of each neuron in it, that is then send to all neurons in preceding (following in the direction of [[gls:bp][BP]]) layer.


*** TODO Advantages of CNN
    # TODO: Find out what I meant by this!!
    # Number of parameters
    # computational demand
    To further highlight the difference between [[gls:fcnn][FCNN]] and [[gls:cnn][CNN]] it is worth to compare the case of 2 neighboring layers.
    Let's have a gray scale input image of size $32 \times 32$ pixels and following layer will be convolutional with 6 feature maps of size 28x28. Kernels used in this convolutional layer will have the size of $5 \times 5$. In this case we have totally $(5 \times 5 + 1) \times 6 = 156$ parameters between the two layers.
    If we would like to create equivalent connection between two layers of [[glspl:fcnn][FCNN]], then it would have mean $(32 \times 32 + 1) \times 28 \times 28 = 803600$ connections (parameters). Which means that difference between the two is of ~5000 ratio.
    This difference would rise exponentially with larger images or with more color channels. When input size of the image changes to 64x64 and it has [[gls:rgb][RGB]] color then [[glspl:fcnn][FCNN]] would requires $(64 \times 64 \times 3 + 1) \times 28 \times 28 = 9634576$ connections (parameters). In the same case the [[gls:cnn][CNN]] only needs $(5 \times 5 \times 3 + 1) \times 6 = 456$ parameters. Which is difference of ~20000 factor.
    Just to elaborate, in case that [[gls:cnn][CNN]] would be used to process video. Analogically to previous examples in case of moving image in time the number of parameters raises linearly with number of images in analyzed video.

** TODO Complexity Control for Neural Networks
   Control of complexity applies to both [[gls:nn][NN]] and [[gls:cnn][CNN]].

   # *** TODO Early stoppage

   # *** TODO Weight decay

*** Dropout
    By far the best regularization method is to combine predictions of many different models. Exactly on this idea are based [[glspl:ensemble model][ensemble model]]. The problem with [[glspl:ensemble model][ensemble model]] is that they are computationally expensive. Because of this, ensembles are usually composed of many very simple models \cite{article__srivastava__2014}.

    This idea is especially problematic with [[glspl:dnn][DNN]], which are model with many parameters that are difficult to train. Moreover, even when trained models are available in some applications it still isn't feasible to evaluate many different models in production environment. Another problem is that there might not be enough data to train these different models.

    All of these problems can be solved by dropout technique. The basic idea is that each neuron in the network has certain probability to be deactivated during one iteration. This potential for deactivation is evaluated in every iteration, to ensure that network has different architecture every time. Deactivated means that it will not propagate any signal through.

    Probability for deactivation is a hyper-parameter that can be tuned, but reasonable default value is 0.5. Dropping out is only happening in the training phase. In testing phase are all weight connection multiplied by the probability of a dropout. This is done because the activation of the network has to stay roughly equivalent[fn:2] in both training ant testing phase. Basic concept is illustrated in Figure [[fig:dropout]]

#+NAME: fig:dropout
#+CAPTION: Dropout: (a) Standard fully connected network. (b) Network with some neurons deactivated. (c) Activation of neuron during training phase. (d) Activation of neuron during testing phase.
[[./img/figure__3__dropout.png]]

[fn:2] For example, when the dropout probability is 0.5, approximately half of the neurons in the network will be deactivated. Therefore in the testing phase the activation would be twice as big.

* Neural Networks
  This chapter is dedicated to description of [[gls:nn][NN]] in general and its special type called [[gls:cnn][CNN]].

** History
   History of [[glspl:nn][NN]] can be arguably dated from 1943, when Warren Mcculloch and Walter Pitts devised mathematical model inspired by Biology of central nervous systems of mammals \cite{article--mcculloch-pitts--1943}.

   This inspired the invention of [[gls:perceptron][Perceptron]], created in 1958 by Frank Rosenblatt. Perceptron used very simple model mimicking biological neuron that was based on mathematical model of Pitts and Mcculloch. Definition of the [[gls:perceptron][Perceptron]] model also described an algorithm for direct learning from data.

   In the beginning [[gls:perceptron][Perceptron]] seemed very promising, but it was soon discovered that it had severe limitations. Most prominent voice of criticism was Marvin Minsky. Minsky published book in which he laid out a case that [[gls:perceptron][Perceptron]] model was unable to solve complex problems \cite{book--minsky-papert--1969}. Among others the book contained mathematical proof that [[gls:perceptron][Perceptron]] is unable to solve simple XOR problem. More generally the [[gls:perceptron][Perceptron]] is only capable of solving linearly separable problems. Even though according to Minsky this criticism wasn't malicious, it in effect stifled the interest in [[glspl:nn][NN]] for over a decade.

   Interest in [[glspl:nn][NN]] was rejuvenated in the early 80's, when it was shown that any previously raised deficiencies could have been solved by usage of multiple units. This was later exacerbated by invention of [[gls:bp][BP]] learning algorithm, which enabled the possibility to gather neurons into groups called layers, which can be stacked into hierarchical structures to form a network. [[Gls:nn][NN]] of this type were commonly called [[gls:mlp][MLP]].

   In 80s and 90s the interest in [[glspl:nn][NN]] plateaued again and general research of [[gls:ai][AI]] was more focused on other[fn:1] machine learning techniques. In the realm of [[gls:classification][classification]] problems, it were notably [[gls:svm][SVM]] and [[gls:ensemble model][Ensemble model]]. [[Gls:ai][AI]] research community also developed several other paradigms of [[glspl:nn][NN]] that were similarly inspired by Biology of certain aspect of central nervous system but took different approaches. Most important examples were [[gls:som][SOM]] and [[gls:rnn][RNN]] (e.g. [[glspl:hopfield network][Hopfield Networks]]).

   By the year 2000, there was very few research groups that were devoting enough attention to the [[glspl:nn][NN]]. There was also certain disdain for [[glspl:nn][NN]] in academia and [[gls:ai][AI]] research community. Success of [[glspl:nn][NN]] that was promised almost half a century ago was finally encountered around 2009, when the first networks with large number of [[glspl:hidden layer][hidden layer]] were successfully trained. This led to mainstream adaption of umbrella term [[gls:deep learning][DL]] which by and large refers to [[gls:dnn][DNN]]. The word deep indicates that networks have large number of [[glspl:hidden layer][hidden layer]].

   The key theoretical insight was to learn complicated functions that could represent high-level abstractions (e.g. vision recognition, language understanding etc.). There is a need for deep architecture.

   [[glspl:nn][NN]] in the times before [[glspl:dnn][DNN]] had only 1 or 2 [[glspl:hidden layer][hidden layer]]. These are today often called shallow networks. Typical Deep Networks can have number of [[glspl:hidden layer][hidden layer]] in order of tens, but in some cases even hundreds \cite{article--He-Zhang--2015}

   Even though that progress of Neural Network into direction of structures with high number of [[glspl:hidden layer][hidden layer]] was obvious, its training was unsolved technical problem for very long time. There were basically 3 reasons why this breakthrough didn't come sooner.
   1. There were no technique allowing the number of [[glspl:hidden layer][hidden layer]] to scale.
   2. There wasn't enough of labeled data necessary to train the [[gls:nn][NN]].
   3. The computational hardware wasn't powerful enough to train sufficiently large and complex networks effectively.

   First problem was tackled by invention of [[glspl:cnn][CNN]] \cite{article--lecun--1989}.
   Second problem was solved simply when there was more data available. This was mainly achieved thanks to effort of large companies (Google, Facebook, YouTube, etc.) but also with help of large community of professionals and hobbyists in data sciences.

   # TODO: maybe make sure that you still stay behind this!
   Both innovation in computational hardware and improvement of training methods were needed to solve the third problem. One of the technical breakthroughs was utilization of [[glspl:gpu][GPU]] for the demanding computation involved in training of a complex network. Thanks to the fact that training process of [[glspl:nn][NN]] is typically large number of simple consequent computations, there is a great potential for parallelization.

[fn:1] These models were usually less complex than [[glspl:nn][NNs]]

** Structure of Neural Networks
   The term [[Gls:nn][NN]] is very general and it describes broad family of models. In this context [[gls:nn][NN]] is distributed and parallel model that is capable of approximating complex non-linear functions. Network is composed from multiple computational units called neurons assembled in particular topology.

   Description of [[gls:nn][NN]] structure will follow the convention laid out in the description of learning algorithm. Meaning that a description of the learning algorithm is composed of model, cost function and optimization procedure. The difference comes into play with the fact that model of [[gls:nn][NN]] is much more complex than the model [[gls:linear regression][linear regression]]. Therefore, the analysis is divided into model of neuron and topology of the network.

*** Model of Neuron
    Neuron is computational unit performing nonlinear transformation of its inputs
    \begin{equation} \label{eq:neuron}
    y = g(\boldsymbol{w}^T\boldsymbol{x} + b).
    \end{equation}

    Argument $\boldsymbol{w}^T\boldsymbol{x} + b$ of function $g$ is often regarded as $z$. Therefore, the equation can be rewritten as
    \begin{equation}
    y=g(z).
    \end{equation}
    Typical schema is shown on Figure [[fig:neuron_model]], which depicts inputs, weights bias and activation function.

    #+NAME: fig:neuron_model
    #+CAPTION: Model of artificial neuron \cite{image__neuron_model}.
    [[./img/figure__2__neuron_model.png]]

    As it was already mentioned model of neuron was inspired by biology. First attempts to create model of neuron had multiple elements equivalent with neurons of human brain. As research progressed this equivalence ceased being as important and modern [[gls:nn][NN]] models correspond to their biological counterparts only superficially.

**** Inputs
     Each neuron has multiple inputs $\boldsymbol{x}$ that are combined together to execute some operation. Each input has designated weight assigned to it.

**** Weights
     Inputs of a neuron are weighted by parameters $\boldsymbol{w}$ that are modified during learning process. Each weight gives strength to each individual input into the neuron. The basic idea is that when the weight is small the particular input doesn't influence the output of the neuron very much. Its influence is large in the opposite case.

**** Bias
     <<sec:bias>>
     Another modifiable parameter is bias $b$ that controls influence of the neuron as a whole.

**** Activation Function
     For [[gls:nn][NN]] to approximate nonlinear function each neuron has to perform nonlinear transformation of its input. This is done with [[gls:af][AF]] $g(z)$ that preforms nonlinear transformation. There are several different commonly used activation functions. Its usage depends on the type of network and also on the type of layer in which they operate.

     One of the oldest and historically most commonly used [[gls:af][AF]] if [[gls:sigmoid][sigmoid]] function. It is defined by
     \begin{equation}
     g(z)=\frac {1} {1+e^{-z}}.
     \end{equation}
     Problem with [[gls:sigmoid][sigmoid]] is that its gradient becomes really flat on both extremes and as such it slows down the learning process \cite{article--krizhevsky--2012}.

     Another activation function is [[gls:hyperbolic tangent][hyperbolic tangent]].  It is defined as
     \begin{equation}
     g(z)=tanh(-z).
     \end{equation}
     Hyperbolic tangent function is less common in feed forward [[gls:nn][NN]], but it is largely used in [[gls:rnn][RNN]].

     Currently most frequently used activation function is [[gls:relu][ReLU]]. It is very commonly used in both convolutional and fully connected layers. It is defined by
     \begin{equation}
     g(z)=\max \{0,z\}.
     \end{equation}
     It has a drawback because it is not differentiable for $z=0$, but it is not a problem in software implementation and one of its biggest advantages is that it can learn very quickly.

     All three activation functions are illustrated in Figure \ref{fig:activation}.

     #+INCLUDE: activation_function.org

*** Topology of the Network
    # TODO: feed-forward and recurrent below should be glossaries!!!
    There are several different commonly used topologies. Two most commonly used in [[gls:deep learning][deep learning]] are feed-forward and recurrent. Feed forward networks are characterized by the fact that during activation the information flows only in forward direction from inputs to output. A recurrent network has some sort of feedback loop.

    Another criterion of topology is how are individual neurons in the network connected. Most commonly are [[glspl:nn][NN]] ordered in layers. In each layer there can be from /1/ to /n/ neurons. Layers are hierarchically stacked. In typical terminology the first layer is called input layer, the last layer is called output layer and the layers in-between are called hidden.

    Description of the network rests on interconnections between individual layers. Most common scheme is called fully connected where each neuron in hidden layer $l$ has input connections from all neurons from previous layer $l-1$ and its output is connected to input of each neuron in following $l+1$ layer. Entire structure is illustrated on Figure [[fig:net_structure]].

    From this point on the term [[gls:nn][NN]] will refer to Feed-forward Fully Connected Neural Network.

    #+NAME: fig:net_structure
    #+CAPTION: Fully connected Feed Forward Neural Network \cite{image__net_structure}.
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__net_structure.png]]

    Types of neurons are dependent on the type of the layer. Currently the main difference is in their [[gls:af][af]], which wasn't the case for a long time. Historically all layers had neurons with [[gls:sigmoid][sigmoid]] [[gls:af][AF]]. It was mainly because the output [[gls:sigmoid][sigmoid]] layer can be easily mapped onto probability distribution, since it acquires vales between 0 and 1. Only relatively recently[fn:2] it was found that network composed of neurons with [[gls:relu][ReLU]] [[gls:af][AFs]] in the hidden layers can be trained very quickly and are more resistant against over-fitting. [[Glspl:af][AF]] are still subject of ongoing research.

    Neurons in output layer need output that can produce probability distribution that can be used to estimate the probability of individual classes. For this reason, most commonly used [[gls:af][AF]] of output neuron is called [[gls:softmax][softmax]]. [[Gls:softmax][Softmax]] is normalized exponential function. It is used to represent probability of an instance being member of class $j$ as
    \begin{equation} \label{eq:softmax}
    g(z)_j = \frac {e^{z_j}} {\sum_{K}^{k=1} {e^{z_k}}},
    \end{equation}
    where $K$ is total number of classes.


[fn:2] In the last decade which is relatively recently in the grand scheme of [[gls:nn][NN]] history.

*** Cost Function
    [[Glspl:cost function][Cost functions]] of [[glspl:nn][NN]] is a complex topic that exceeds scope of this thesis. One of the most common cost function used in [[glspl:nn][NN]] for [[gls:classification][classification]] into multiple classes is [[gls:categorical cross entropy][categorical cross entropy]]. For [[gls:softmax][softmax]] [[gls:af][AF]] from Equation \ref{eq:softmax} is [[gls:cost function][cost function]] defined as
    \begin{equation}
    C = - \frac {1}{n} \sum_{i=1}^{n} y^{(i)}\ln g(z^{(i)}) + (1 - y^{(i)}) \ln (1 - g(z^{(i)})),
    \end{equation}
    where $y^{(i)}$ if correct class of the instance and $n$ is total number of instances.

*** Optimization Procedure
    Every optimization procedure for [[gls:nn][NN]] is based on gradient descent. In other words, it is iterative process that aims to lower training error of the network by differentiating of [[gls:cost function][cost function]] and adjusting parameters $\boldsymbol{\theta}$ of the model by following the negative gradient.

    The problem is that [[gls:cost function][cost function]] of entire network is very complex and has many parameters. To find the gradient of the cost function it is necessary to go through all of the units in the network and estimate their contribution to the overall error. Technique that is used to solve this problem is called [[gls:bp][BP]]. [[Gls:bp][BP]] if often confused to be complete learning algorithm which is not the case, it is only the method to compute the gradient \cite{book--goodfellow--2016}.

**** Back-propagation
     <<sec:back-prop>>
     To estimate the influence of individual units in a network the [[gls:bp][BP]] is used to compute delta $\delta_j^l$, where $l$ is layer and $j$ is index of neuron in that layer. Algorithm starts at the output of [[gls:nn][NN]], more specifically its cost function.
     \begin{equation}
     \delta^L = \nabla_x C \odot g^{'} (z^L)
     \end{equation}
     where $L$ is last layer of the network and $\nabla_x C$ is gradient of cost function with respect to $x$ and $\odot$ is the Hadamard product[fn:3].

     In subsequent lower layers the deltas are computed as
     \begin{equation} \label{eq:deltas}
     \delta^l = ((\boldsymbol{w}^{l+1})^T \delta^{l+1} \odot g^{'} (z^l)
     \end{equation}
     where $(\boldsymbol{w}^{l+1})^T$ is from Equation \ref{eq:neuron}.

     Each neuron has two modifiable parameters $b$ and $\boldsymbol{w}$. To estimate the rate of change for parameter $b_j^l$ from Equation \ref{eq:neuron} it needs to be computed as
     \begin{equation}
     \frac{\partial C} {\partial b_j^l} = \delta_j^{l}
     \end{equation}

     Change of weight $w_{jk}^l$ from Equation \ref{eq:neuron} it needs to be computed as
     \begin{equation}
     \frac{\partial C} {\partial w_{jk}^l} = x^{l-1}\delta_j^l
     \end{equation}

[fn:3] It is element-wise product of matrices.

**** Gradient Descent Optimization
     [[Gls:bp][BP]] estimates gradient of all modifiable parameters $b$ and $\boldsymbol{w}$ in the network. These parameters can be referred to by vector $\boldsymbol{\theta}$. Therefore, the gradient of the function to be minimized can be written as $\nabla_{\boldsymbol{\theta}_{t-1}} f(\boldsymbol{\theta_{t-1}})$.

     Simplest learning algorithm is called [[gls:gradient descent][gradient descent]]. Even though simple, it is very robust learning algorithm.
     \begin{align} \label{eq:sgd}
     \boldsymbol{g}_t        & \leftarrow \nabla_{\boldsymbol{\theta}_{t-1}} f(\boldsymbol{\theta}_{t-1}) \\
     \boldsymbol{\theta}_{t} & \leftarrow \boldsymbol{\theta}_{t-1} - \eta \boldsymbol{g}_t
     \end{align}
     Algorithm has one meta-parameter $\eta$, which is often called learning rate. It determines how quickly are $\boldsymbol{\theta}$ parameters updated. Simple gradient descent has the shortcoming that update of parameters is always exactly proportional to change of gradient. This might become a problem when the gradient change slows down. This algorithm is also often called [[gls:sgd][SGD]]. The word stochastic indicates that during training the algorithm is using random selection of instances to train.

     There are many different variations on the gradient descent method. Following definitions are taken from \cite{article--dozat--2015}.
***** Adam
      It is more complex learning algorithm that combines $L_2$ norm and classical momentum based optimization. It should converge faster than classical Gradient Descent.
      \begin{align} \label{eq:adam}
      \boldsymbol{g}_t        & \leftarrow \nabla_{\boldsymbol{\theta}_{t-1}} f(\boldsymbol{\theta}_{t-1}) \\
      \boldsymbol{m}_t        & \leftarrow \mu \boldsymbol{m}_{t-1} + (1- \mu) \boldsymbol{g}_t \\
      \hat{\boldsymbol{m}_t}  & \leftarrow \frac {\boldsymbol{m}_t} {1 - \mu^t} \\
      \boldsymbol{n}_t        & \leftarrow \nu \boldsymbol{n}_{t-1} + (1 - \nu) \boldsymbol{g}_{t}^{2} \\
      \hat{\boldsymbol{n}_t}  & \leftarrow \frac {\boldsymbol{n}_t} {1 - \nu^t} \\
      \boldsymbol{\theta}_{t} & \leftarrow \boldsymbol{\theta}_{t-1} - \eta \frac {\hat{\boldsymbol{m}_t}} {\sqrt{\boldsymbol{n}_t} + \varepsilon}
      \end{align}

***** Nadam
      Nadam is further improvement of Adam that extends it with Nesterov acceleration trick that should in most cases improve speed of convergence \cite{article--dozat--2015}.
      \begin{align} \label{eq:nadam}
      \boldsymbol{g}_t        & \leftarrow \nabla_{\boldsymbol{\theta}_{t-1}} f(\boldsymbol{\theta}_{t-1}) \\
      \hat{\boldsymbol{g}_t}  & \leftarrow \frac {\boldsymbol{g}_t} {1 - \prod_{i=1}^{t} \mu_i} \\
      \boldsymbol{m}_t        & \leftarrow \mu \boldsymbol{m}_{t-1} + (1 - \mu) \boldsymbol{g}_t \\
      \hat{\boldsymbol{m}_t}  & \leftarrow \frac {\boldsymbol{m}_t} {1 - \prod_{i=1}^{t} \mu_i} \\
      \boldsymbol{n}_t        & \leftarrow \nu \boldsymbol{n}_{t-1} + (1 - \nu) \boldsymbol{g}_{t}^{2} \\
      \hat{\boldsymbol{n}_t}  & \leftarrow \frac {\boldsymbol{n}_t} {1 - \nu^t} \\
      \bar{\boldsymbol{m}_t}  & \leftarrow (1 - \mu_t) \hat{\boldsymbol{g}_t} + \mu_{t+1} \hat{\boldsymbol{m}_t} \\
      \boldsymbol{\theta}_{t} & \leftarrow \boldsymbol{\theta}_{t-1} - \eta \frac {\bar{\boldsymbol{m}_t}} {\sqrt{\boldsymbol{n}_t} + \varepsilon}
      \end{align}

*** TODO Shortcomings of Neural Network in Image Processing        :noexport:
    It was found that general [[gls:fcnn][FCNN]] is not ideal for image processing needs. Even small images typically represents enormous amount of inputs (i.e. image of the size $64 \times 64$ pixels represents 4096 inputs).

    Since each of these inputs has to be connected to all neurons in following layer and weight of each connection has to be memorized, this represents enormous amount of parameters.

    The structure of [[gls:fcnn][FCNN]] has another deficiency for image processing application, which is that it doesn't capture geometric properties of information from input image. In other words, because individual layers are fully connected (each output in lower layer is connected to each input in higher layer) networks are not capturing any information about relation of position of individual pixels in the image.

    Third problem is that for higher depth of [[gls:fcnn][FCNN]] increases the likelihood of getting stuck in some local minima.

    All of these problems were solved by the specific type of [[gls:nn][NN]] model called [[glspl:cnn][CNN]] \cite{article--lecun--1989}.

** Convolutional Neural Networks
   [[glspl:cnn][CNN]] are specialized type of [[glspl:nn][NN]] that was originally used in image processing applications. They are arguably most successful models in [[gls:ai][AI]] inspired in biology. Even though they were guided by many different fields, the core design principles were drawn from neuroscience. Since their success in image processing, they were also very successfully deployed in natural language and video processing applications.

   Aforementioned inspiration in biology was based on scientific work of David Hubel and Torsten Wiesel. Neurophysiologists Hubel and Wisel, investigated vision system of mammals from late 1950 for several years. In the experiment, that might be considered little gruesome for today's standards, they connected electrodes into brain of anesthetized cat and measured brain response to visual stimuli \cite{article--hubel--1959}. They discovered that reaction of neurons in visual cortex was triggered by very narrow line of light shined under specific angle on projection screen for cat to see. They determined that individual neurons from visual cortex are reacting only to very specific patterns in input image. Hubel and Wiesel were awarded the Nobel Prize in Physiology and Medicine in 1981 for their discovery.

   In the following text is presumed that convolutional layer is working with rectangular input data (e.g. images). Even though the Convolutional networks can also be also used to classify 1-dimensional[fn:4] or 3-dimensional [fn:5] input.

[fn:4] For example sound signal.
[fn:5] For example \acrshort{ct} scans.

*** Structure of CNN
    Structure of Convolutional networks is typically composed of three different types of layers. Layer can be either Convolutional, Pooling or  [[gls:fc][FC]]. Each type of layer has different rules for forward and error backward signal propagation.

    #+NAME: fig:cnn_structure
    #+CAPTION: Typical structure of Convolutional Neural Network \cite{image__cnn_structure}
    #+ATTR_LATEX: :width 4in
    [[./img/figure__2__cnn_structure.png]]


    There are no precise rules on how the structure of individual layers should be organized. However with exception of recent development[fn:6] [[glspl:cnn][CNN]] are typically structured in two parts. First part, usually called feature extraction, is using combinations of convolutional and pooling layers. Second part called classification is using fully connected layers. This is illustrated in Figure [[fig:cnn_structure]].

[fn:6] GoogLeNet described in section [[sec:ilsvrc]].

**** Convolutional layer

     As the name suggests this layer employs convolution operation. Input into this layer is simply called input. Convolution operation is performed on input with specific filter, which is called [[gls:kernel][kernel]]. Output of convolution operation is typically called [[gls:feature map][feature map]].

     Input into Convolutional layer is either image (in case of first network layer) or [[gls:feature map][feature map]] from previous layer. [[Gls:kernel][kernel]] is typically of square shape and its width can range from 3 to N pixels. [[Gls:feature map][feature map]] is created by convolution of [[gls:kernel][kernel]] over each specified element of input. Convolution is described in more detail in section describing training of [[gls:cnn][CNN]].

     Depending on the size of [[gls:kernel][kernel]] and layer's padding preferences the process of convolution can produce [[gls:feature map][feature map]] of different size than input. When the size of output should be preserved it is necessary to employ [[gls:zero padding][zero padding]] on the edges of input. [[Gls:zero padding][zero padding]] in this case has to add necessary amount of zero elements around the edges of input. This amount is determined by
     \begin{equation}
     p = ((h - 1) / 2)
     \end{equation}
     where $h$ is width of used [[gls:kernel][kernel]]. In opposite case the [[gls:feature map][feature map]] is reduced by the $2p$. Reduction of size of the [[gls:feature map][feature map]] can be in some cases desirable. [[Gls:zero padding][zero padding]] is illustrated on Figure [[fig:zero_padding]].

     #+NAME: fig:zero_padding
     #+CAPTION: A zero padded 4x4 matrix \cite{image__zero_padding}
     #+ATTR_LATEX: :width 4in
     [[./img/figure__2__zero_padding.png]]

     Reduction of [[gls:feature map][feature map]] can go even further in case of use of [[gls:stride][stride]]. Application of [[gls:stride][stride]] specifies by how many input points is traversed when moving to neighboring position in each step. When the [[gls:stride][stride]] is 1, [[gls:kernel][kernel]] is moved by 1 on each step and the resulting size of [[gls:feature map][feature map]] is not affected.

     Each Convolutional layer is typically composition of several different kernels. In other words, output of this layer is tensor containing [[gls:feature map][feature map]] for each used [[gls:kernel][kernel]]. Each of these is designed to underline different features of input image. In the first layers these features are typically edges. Higher the layer, the more complex features are captured.

     Each [[gls:kernel][kernel]] that is used is applied to all inputs of the image to produce one [[gls:feature map][feature map]] which basically means that neighboring layers are sharing the same weights. This might not be sufficient in some applications and therefore it is possible to use two other types of connections. [[Gls:locally connected][Locally connected]] which basically means that applied [[gls:kernel][kernel]] is of the same size as the input and [[gls:tiled convolution][tiled convolution]] which means alternation of more than one set of weights on entire input.

     [[Gls:tiled convolution][tiled convolution]] is interesting because with clever combination with [[gls:max-pooling][max-pooling]] explained bellow it allows to train specific feature from multiple angles (in other words invariant to rotation).

     Each convolutional layer has non-linearity on its output that is sometimes also called the [[gls:detector stage][detector stage]]. This is equivalent to [[gls:af][AF]] of [[glspl:nn][NN]]. [[Gls:af][AF]] of [[gls:cnn][CNN]] is commonly [[gls:relu][RELU]].

**** Pooling layer
     <<sec:pooling>>

     This layer typically doesn't constitute any learning process but it is used to down-sample size of the input. The Principle is that input is divided into multiple not-overlapping rectangular elements and units within each element are used to create single unit of output. This decreases the size of output layer while preserving the most important information contained in input layer. In other words, pooling layer compresses information contained within input.

     Type of operation that is performed on each element determines a type of pooling layer. This operation can be averaging over units within element, selecting maximal value from element or alternatively learned linear combination of units within element. Learned linear combination introduces form of learning into the pooling layer, but it is not very prevalent.

     Selecting of maximal value is most common type of pooling operation and in that case the layer is called [[gls:max-pooling][max-pooling]] accordingly. Positive effect of Max-pooling down-sampling is that extracted features that are learned in convolution are invariant to small shift of input. Principle of [[gls:max-pooling][max-pooling]] is illustrated on Figure [[fig:max_pooling]].

     #+NAME: fig:max_pooling
     #+CAPTION: Principle of Max-pooling \cite{image__max_poolig}
     #+ATTR_LATEX: :width 4in
     [[./img/figure__4__max_pooling.png]]

     As already mentioned another advantage of Max-pooling arises when combined with [[gls:tiled convolution][tiled convolution]]. To create simple detector that is invariant to rotation it possible to use 4 different kernels that are rotated by 90 degrees among each other and when the [[gls:tiled convolution][tiled convolution]] is used to tile them in groups of 4, the Max-pooling makes sure that resulted [[gls:feature map][feature map]] contains output from the [[gls:kernel][kernel]] with strongest signal (i.e. the one trained for that specific rotation of the feature).

     [[Gls:max-pooling][max-pooling]] layer will be used to describe process of training of [[glspl:cnn][CNN]].

     # TODO: Pooling can also have stride!!!

**** Fully-Connected layer

     Fully-Connected layer is identical to layer from [[gls:fcnn][FCNN]] that was already described. Its training also follows already described process.

*** TODO Training of CNN
    Optimization process of [[gls:cnn][CNN]] is analogues to [[gls:fcnn][FCNN]]. Situation with [[gls:cnn][CNN]] is more complicated because network is composed of different types of layers. Forward signal propagation and backward error propagation are following special rules for each layer. Equations used in this section were inspired from \cite{online--gibiansky--2014}.

    First phase is called forward-propagation, where the signal is propagated from inputs of the [[glspl:cnn][CNN]] to its output. In the last layer the output is compared with desired value by [[gls:cost function][cost function]] and error is estimated. In second phase is again used [[gls:bp][BP]] algorithm to estimate error contribution of individual units. Variable parameters of the network are again optimization by [[gls:gradient descent][gradient descent]] algorithm.

**** Forward Propagation of Convolution Layer
      Each convolutional layer is preforming convolution operation on its input. Presuming that input of a layer is of size $N \times N$ units and [[gls:kernel][kernel]] is of size $m \times m$. Convolution is computed over $(N-m+1) \times (N-m+1)$ units without zero padding.

      Computation of convolution output $x_{ij}^{l}$ is defined as
      \begin{equation} \label{eq:forward_conv}
      x_{ij}^{l}=\sum_{a=0}^{m-1}\sum_{b=0}^{m-1}\omega_{ab}y_{(i+a)(j+b)}^{l-1}
      \end{equation}
      where $i, j \in (0,N-m+1)$, l is index of current layer, $\omega_{ab}$ are weights of the [[gls:kernel][kernel]] and $y_{(i+a)(j+b)}^{l-1}$ is output of previous layer.

      Output of convolutional layer $y_{ij}^{l}$ is computed by squashing of output of convolution operation $x_{ij}^{l}$ through non-linearity:

      \begin{equation}
      y_{ij}^{l} = g(x_{ij}^{l})
      \end{equation}
      where $g$ represents this non-linear function.

**** Backward Propagation of Convolution Layer
     Backward propagation for convolutional layer is following the same principles as described in Section [[sec:back-prop]]. The difference is in fact that convolution [[gls:kernel][kernel]] shares weights for entire layer and kernels do not have bias described in Section [[sec:bias]].

     Given partial derivative of error from previous layer with respect to output of convolutional layer $\frac{\partial C} {\partial y_{ij}^{l}}$, influence of kernel weights on the [[gls:cost function][cost function]] it needs to computed
     \begin{equation}
     \frac{\partial C} {\partial \omega_{ab}}
     =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial C}{\partial x_{ij}^{l}} \frac{\partial x_{ij}^{l}} {\partial \omega_{ab}}.
     \end{equation}
     From Equation \ref{eq:forward_conv} it follows that $\frac {\partial x_{ij}^{l}} {\partial \omega_{ab}} = y_{(i+a)(j+b)}^{l-1}$, thus
     \begin{equation}
     \frac{\partial C} {\partial \omega_{ab}}
     =\sum_{i=0}^{N-m} \sum_{j=0}^{N-m} \frac{\partial C}{\partial x_{ij}^{l}} y_{(i+a)(j+b)}^{l-1}.
     \end{equation}
     To compute deltas (equivalent to Equation \ref{eq:deltas}) $\frac{\partial C} {\partial x_{ij}^{(l)}}$ using the chain rule
     \begin{equation}
     \frac{\partial C} {\partial x_{ij}^{(l)}}
     =\frac{\partial C} {\partial y_{ij}^{l}} \frac{\partial y_{ij}^{l}} {\partial x_{ij}^{l}}
     =\frac{\partial C} {\partial y_{ij}^{l}} \frac{\partial} {\partial x_{ij}^{l}} \left( g' \left(x_{ij}^{l}\right) \right)
     =\frac{\partial C} {\partial y_{ij}^{l}} g' \left( x_{ij}^{l} \right)
     \end{equation}
     Since $\frac{\partial C} {\partial y_{ij}^{l}}$ is already given the deltas is computed by derivation of [[gls:af][AF]]. Last step comes to propagation of error into previous layer by equation
     \begin{equation} \label{eg:back_prop}
     \frac{\partial C} {\partial y_{ij}^{l-1}}
     =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial C} {\partial x_{(i-a)(j-b)}^{l}} \frac{\partial x_{(i-a)(j-b)}^{l}} {\partial  y_{ij}^{l-1}}
     \end{equation}
     Again from Equation \ref{eq:forward_conv} it follows that $\frac{\partial x_{(i-a)(j-b)}^{l}} {\partial  y_{ij}^{l-1}} = \omega_{ab}$, therefore
     \begin{equation}
     \frac{\partial C} {\partial y_{ij}^{l-1}}
     =\sum_{a=0}^{m-1} \sum_{b=0}^{m-1} \frac{\partial C} {\partial x_{(i-a)(j-b)}^{l}} \omega_{ab}.
     \end{equation}
     The result looks suspiciously similar to convolution operation and can be interpreted as convolution of error with flipped kernel.

**** Forward Propagation of Pooling layer
     Feed forward operation of pooling layer is strait forward as described in section [[sec:pooling]]. Ratio is typically $4$ to $1$, which means that input matrix is divided into not-overlapping sub-matrices of size $2 \times 2$ and each of these produces 1 output. Another possibility is to have overlapping sub-matrices, where size of sub-matrix is larger the number of pixels between application of pooling.

**** Backward Propagation of Pooling Layer
     As mentioned in section for forward-propagation, there is no explicit learning process happening in pooling layer. Error is propagated backwards depending on how the signal was propagated forward. In case of [[gls:max-pooling][Max-Pooling]] layer the error is propagated only to the unit with maximal output in forward-propagation phase (in other words to the winner of pooling). The error is propagated very sparsely, as result.


*** TODO Advantages of CNN                                         :noexport:
    # TODO: Find out what I meant by this!!
    # Number of parameters
    # computational demand
    To further highlight the difference between [[gls:fcnn][FCNN]] and [[gls:cnn][CNN]] it is worth to compare the case of 2 neighboring layers.
    Let's have a gray scale input image of size $32 \times 32$ pixels and following layer will be convolutional with 6 feature maps of size $28 \times 28$. Kernels used in this convolutional layer will have the size of $5 \times 5$. In this case we have totally $(5 \times 5 + 1) \times 6 = 156$ parameters between the two layers.
    If we would like to create equivalent connection between two layers of [[glspl:fcnn][FCNN]], then it would have mean $(32 \times 32 + 1) \times 28 \times 28 = 803600$ connections (parameters). Which means that difference between the two is of 5000 ratio.
    This difference would rise exponentially with larger images or with more color channels. When input size of the image changes to 64x64 and it has \acrshort{rgb} color then [[glspl:fcnn][FCNN]] would requires $(64 \times 64 \times 3 + 1) \times 28 \times 28 = 9634576$ connections (parameters). In the same case the [[gls:cnn][CNN]] only needs $(5 \times 5 \times 3 + 1) \times 6 = 456$ parameters. Which is difference of 20000 factor.
    Just to elaborate, in case that [[gls:cnn][CNN]] would be used to process video. Analogically to previous examples in case of moving image in time the number of parameters raises linearly with number of images in analyzed video.

** TODO Regularization of Neural Networks
   Control of complexity applies to both [[gls:nn][NN]] and [[gls:cnn][CNN]]. There are several popular regularization techniques that mostly consist of modification of [[gls:cost function][cost function]] or optimization algorithm. Slightly different approach is to modify structure of the network during training phase.

**** Dropout
     By far the best regularization method is to combine predictions of many different models. This method greatly improves generalization ability of combined model while preventing over-fitting. Exactly on this idea are based [[glspl:ensemble model][ensemble model]]. The problem with [[glspl:ensemble model][ensemble model]] is that they are computationally expensive. Because of this, ensembles are usually composed of many very simple models \cite{article--srivastava--2014}.

     This idea is especially problematic with [[glspl:dnn][DNN]], which are model with many parameters that are difficult to train. Moreover, even when trained models are available in some applications it still isn't feasible to evaluate many different models in production environment. Another problem is that there might not be enough data to train these different models.

     All of these problems can be solved by dropout technique. The basic idea is that each neuron in the network has certain probability to be deactivated during one iteration. This potential for deactivation is evaluated in every iteration, to ensure that network has different architecture every time. Deactivated means that it will not propagate any signal through. This forces individual neurons to learn features that are less dependent on its surrounding.

     Probability for deactivation is a hyper-parameter that can be tuned, but reasonable default value is 0.5. Dropping out is only happening in the training phase. In testing phase are all weight connection multiplied by the probability of a dropout. This is done because the activation of the network has to stay roughly equivalent[fn:7] in both training ant testing phase. Basic concept is illustrated in Figure [[fig:dropout]]

     #+NAME: fig:dropout
     #+CAPTION: Dropout: (a) Standard fully connected network. (b) Network with some neurons deactivated. (c) Activation of neuron during training phase. (d) Activation of neuron during testing phase \cite{image__dropout}.
     [[./img/figure__3__dropout.jpg]]

[fn:7] For example, when the dropout probability is 0.5, approximately half of the neurons in the network will be deactivated. Therefore in the testing phase the activation would be twice as big.

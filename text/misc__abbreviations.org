#+LATEX_HEADER: \newacronym{ai}{AI}{Artificial Intelligence}
#+LATEX_HEADER: \newacronym{api}{API}{Application Programming Interface}
#+LATEX_HEADER: \newacronym{cnn}{CNN}{Convolutional Neural Network}
#+LATEX_HEADER: \newacronym{ct}{CT}{Computed tomography}
#+LATEX_HEADER: \newacronym{dnn}{DNN}{Deep Neural Network}
#+LATEX_HEADER: \newacronym{dsp}{DSP}{Digital Signal Processor}
#+LATEX_HEADER: \newacronym{fcnn}{FCNN}{Fully Connected Neural Network}
#+LATEX_HEADER: \newacronym{ffnn}{FFNN}{Feed-Forward Neural Network}
#+LATEX_HEADER: \newacronym{gpu}{GPU}{Graphics Processing Unit}
#+LATEX_HEADER: \newacronym{hdf5}{HDF5}{Hierarchical Data Format 5}
#+LATEX_HEADER: \newacronym{ilsvrc}{ILSVRC}{Large Scale Visual Recognition Challenge}
#+LATEX_HEADER: \newacronym{lstm}{LSTM}{Long Short Term Memory}
#+LATEX_HEADER: \newacronym{mlp}{MLP}{Multilayer Perceptron}
#+LATEX_HEADER: \newacronym{mri}{MRI}{Magnetic resonance imaging}
#+LATEX_HEADER: \newacronym{nn}{NN}{Neural Network}
#+LATEX_HEADER: \newacronym{pca}{PCA}{Principal Component Analysis}
#+LATEX_HEADER: \newacronym{relu}{ReLU}{Restricted Linear Unit}
#+LATEX_HEADER: \newacronym{rgb}{RGB}{Red Green Blue}
#+LATEX_HEADER: \newacronym{rnn}{RNN}{Recurrent Neural Network}
#+LATEX_HEADER: \newacronym{sgd}{SGD}{Stochastic Gradient Descent}
#+LATEX_HEADER: \newacronym{som}{SOM}{Self Organizing Map}
#+LATEX_HEADER: \newacronym{svd}{SVD}{Singular Value Decomposition}
#+LATEX_HEADER: \newacronym{svm}{SVM}{Support Vector Machines}
#+LATEX_HEADER: \newacronym{knn}{KNN}{K-nearest Neighbors}
#+LATEX_HEADER: \newacronym{cpu}{CPU}{Central Processing Unit}

#+LATEX_HEADER: \newglossaryentry{ensemble models}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={ensemble models},
#+LATEX_HEADER: description={special case of models that is using prediction of multiple different models with the same input data in order to achieve higher accuracy}
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{random forest}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={random forest},
#+LATEX_HEADER: description={ensemble model that is based on combination of bagging decision trees}
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{recursive networks}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={recursive networks},
#+LATEX_HEADER: description={General type of neural networks that contains feedback loop within its structure.}
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{hopfield network}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={hopfield network},
#+LATEX_HEADER: description={Specific type of recurrent neural network invented by John Hopfield in 1982.}
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{regression}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={regression},
#+LATEX_HEADER: description={Machine learning task for approximation of continuous-valued function response}
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{k-means}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={K-means},
#+LATEX_HEADER: description={Unsupervised learning algorithm}
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{perceptron}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={Perceptron},
#+LATEX_HEADER: description={Model of artificial neuron},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{deep learning}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={deep learning},
#+LATEX_HEADER: description={Popularized term that refers to machine learning models that have architecture that contains many layers.},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{deep}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={deep},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{classification}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={classification},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{machine learning}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={machine learning},
#+LATEX_HEADER: description={research study dealing with models able to learn from experience.},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{bp}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={back-propagation},
#+LATEX_HEADER: description={Technique that enables the computation of error distribution throughout the network.},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{linear regression}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={linear regression},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{af}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={activation function},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{hidden unit}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={hidden unit},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{output unit}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={output unit},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{hidden layer}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={hidden layer},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{cost function}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={cost function},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{loss function}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={loss function},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{fc}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={fully connected},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{max-pooling}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={Max-Pooling},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{forward-propagation}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={forward-propagation},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{gradient descent}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={gradient descent},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{sum of square}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={sum of square},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }


#+LATEX_HEADER: \newglossaryentry{feature map}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={feature map},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }


#+LATEX_HEADER: \newglossaryentry{zero padding}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={zero padding},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }


#+LATEX_HEADER: \newglossaryentry{locally connected}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={locally connected},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }


#+LATEX_HEADER: \newglossaryentry{detector stage}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={detector stage},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{tiled convolution}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={tiled convolution},
#+LATEX_HEADER: description={TODO},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{kernel}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={kernel},
#+LATEX_HEADER: description={Filter used by convolutional layer.},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{stride}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={stride},
#+LATEX_HEADER: description={It specifies size of convolution step. Default value is one, which means that kernel is moved by one pixel in each direction.},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{sigmoid}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={sigmoid},
#+LATEX_HEADER: description={Activation function historically usded Perceptron model.},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{under fitting}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={under fitting},
#+LATEX_HEADER: description={Statistical term describing models inferior ability to capture underlying relationship between input and output due to a lack of complexity.},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{over fitting}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={over fitting},
#+LATEX_HEADER: description={Statistical term describing models inferior ability to capture underlying relationship between input and output due to an abundance of complexity.},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{hyperbolic tangent}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={hyperbolic tangent},
#+LATEX_HEADER: description={Activation function commonly used in recurrent neural networs.},
#+LATEX_HEADER: }

#+LATEX_HEADER: \newglossaryentry{softmax}
#+LATEX_HEADER: {
#+LATEX_HEADER: name={softmax},
#+LATEX_HEADER: description={Activation function commonly used output layer of feed forward neural networks.},
#+LATEX_HEADER: }
